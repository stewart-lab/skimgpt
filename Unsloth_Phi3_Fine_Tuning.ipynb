{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stewart-lab/kmGPT/blob/fine-tuning/Unsloth_Lora_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuzyiSTCNQ0l",
    "outputId": "12c9129b-fbf7-4f3c-f178-56415db5aaca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install peft\n",
    "!pip install wandb\n",
    "!pip install trl\n",
    "!pip install bitsandbytes\n",
    "!pip install scikit-learn\n",
    "!pip install \"unsloth[cu118-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install \"unsloth[cu118-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grIXo24axlGk",
    "outputId": "e6f2e82b-f48c-4f67-a2cd-ad9459ba7d28"
   },
   "outputs": [],
   "source": [
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXsaEAu0uBZo"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import wandb\n",
    "import transformers\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from unsloth import FastLanguageModel\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from torch import nn\n",
    "import sys\n",
    "import gc\n",
    "from transformers import AdamW\n",
    "from accelerate import notebook_launcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "import time\n",
    "import re\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments, BitsAndBytesConfig\n",
    "import accelerate\n",
    "import json\n",
    "from peft import IA3Config, IA3Model, LoraConfig\n",
    "import jinja2\n",
    "import math\n",
    "import bitsandbytes as bnb\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import math\n",
    "from trl import setup_chat_format\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# From this Gist: https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1On8ZTnggTen",
    "outputId": "e5a3956c-ea7c-4c73-e0e8-82a00995d995"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzJk4KGHPJR4",
    "outputId": "192227cd-e669-49a2-d121-3488f473d120"
   },
   "outputs": [],
   "source": [
    "!wandb login 4a376fd0ab1c0901b9d9886d0734a88b4794a7fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86pTN-0BdLAy"
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    # General Configuration\n",
    "    device_type = \"gpus\"\n",
    "    model = \"unsloth/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "    # Training Configuration\n",
    "    max_seq_length = 2048\n",
    "    trust = True\n",
    "\n",
    "    # Porpoise One (Relevance Filtering Parameters)\n",
    "    ab_hypothesis = \"There exists an interaction between the disease {a_term} and the gene {b_term}.\"\n",
    "    bc_hypothesis = \"There exists an interaction between the drug {c_term} and the gene {b_term}.\"\n",
    "    ac_hypothesis = \"The drug {c_term} has an interaction with the disease {a_term}.\"\n",
    "\n",
    "    rel_instr = \"Classify this abstract as either 0 (Not Relevant) or 1 (Relevant) for evaluating the provided hypothesis.\"\n",
    "\n",
    "    # Porpoise Two (Supports parameters)\n",
    "    sup_instr = \"Explain why (or why not) this biomedical abstract supports the provided statement. Give a score of 1 for supports and a score of 0 for does not support.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j60QzJeoaOGQ",
    "outputId": "233665df-1f28-4d8a-e8fb-b289b804c56e"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.model,\n",
    "    max_seq_length = config.max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    trust_remote_code = config.trust,\n",
    "    attn_implementation = 'flash_attention_2',\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    r = 16,\n",
    "    # lora_alpha = 32,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_dora = True,\n",
    "    loftq_config = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNRSV5hfQW81"
   },
   "outputs": [],
   "source": [
    "def train_ans_prompt(hyp, abstract, instr, label, cot) -> str:\n",
    "\treturn f\"Abstract: {abstract}\\nHypothesis: {hyp}\\nInstructions: {config.rel_instr}\\nScore: {label}\\nExplanation: {cot}\"\n",
    "\n",
    "def test_ans_prompt(hyp, abstract, instr, label) -> str:\n",
    "\treturn f\"Abstract: {abstract}\\nHypothesis: {hyp}\\nInstructions: {config.rel_instr}\\nScore: {label}\"\n",
    "\n",
    "def eval_ans_prompt(hyp, abstract, instr) -> str:\n",
    "\treturn f\"Abstract: {abstract}\\nHypothesis: {hyp}\\nInstructions: {config.rel_instr}\\nScore: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJrAejGQ0gfB"
   },
   "outputs": [],
   "source": [
    "train_rel = pd.read_csv(\"./data/Porpoise_1/same_dist_train.tsv\", sep=\"\\t\")\n",
    "test_rel = pd.read_csv(\"./data/Porpoise_1/same_dist_test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqvJzlex1qdO"
   },
   "outputs": [],
   "source": [
    "def processRowTrainText(row, prompt_fn):\n",
    "    if pd.isnull(row[\"a_term\"]):\n",
    "        hypothesis = config.bc_hypothesis.format(c_term=row[\"c_term\"], b_term=row[\"b_term\"])\n",
    "    elif pd.isnull(row[\"b_term\"]):\n",
    "        hypothesis = config.ac_hypothesis.format(c_term=row[\"c_term\"], a_term=row[\"a_term\"])\n",
    "    elif pd.isnull(row[\"c_term\"]):\n",
    "        hypothesis = config.ab_hypothesis.format(a_term=row[\"a_term\"], b_term=row[\"b_term\"])\n",
    "    return prompt_fn(hypothesis, row[\"abstract\"], config.rel_instr, int(row[\"label\"]), row[\"cot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f3iNfOSgTeq"
   },
   "outputs": [],
   "source": [
    "def processRowTestText(row, prompt_fn):\n",
    "    if pd.isnull(row[\"a_term\"]):\n",
    "        hypothesis = config.bc_hypothesis.format(c_term=row[\"c_term\"], b_term=row[\"b_term\"])\n",
    "    elif pd.isnull(row[\"b_term\"]):\n",
    "        hypothesis = config.ac_hypothesis.format(c_term=row[\"c_term\"], a_term=row[\"a_term\"])\n",
    "    elif pd.isnull(row[\"c_term\"]):\n",
    "        hypothesis = config.ab_hypothesis.format(a_term=row[\"a_term\"], b_term=row[\"b_term\"])\n",
    "    return prompt_fn(hypothesis, row[\"abstract\"], config.rel_instr, int(row[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFjizwxhpLEW"
   },
   "outputs": [],
   "source": [
    "def processRowPrompt(row, prompt_fn):\n",
    "    if pd.isnull(row[\"a_term\"]):\n",
    "        hypothesis = config.bc_hypothesis.format(c_term=row[\"c_term\"], b_term=row[\"b_term\"])\n",
    "    elif pd.isnull(row[\"b_term\"]):\n",
    "        hypothesis = config.ac_hypothesis.format(c_term=row[\"c_term\"], a_term=row[\"a_term\"])\n",
    "    elif pd.isnull(row[\"c_term\"]):\n",
    "        hypothesis = config.ab_hypothesis.format(a_term=row[\"a_term\"], b_term=row[\"b_term\"])\n",
    "    return prompt_fn(hypothesis, row[\"abstract\"], config.rel_instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GNiRFzM0sf5"
   },
   "outputs": [],
   "source": [
    "train_rel[\"text\"] = train_rel.apply(lambda row: processRowTrainText(row, train_ans_prompt), axis=1)\n",
    "train_rel[\"prompt\"] = train_rel.apply(lambda row: processRowPrompt(row, eval_ans_prompt), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfhPnZBELFTd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_rel[\"text\"] = test_rel.apply(lambda row: processRowTestText(row, test_ans_prompt), axis=1)\n",
    "test_rel[\"prompt\"] = test_rel.apply(lambda row: processRowPrompt(row, eval_ans_prompt), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(row):\n",
    "    return f'Abstract: {row[\"abstract\"]}\\nStatement: {row[\"statement\"]}\\nInstructions: {config.sup_instr}\\nScore: {row[\"label\"]}\\nExplanation: {row[\"cot\"]}'\n",
    "\n",
    "def getPrompt(row):\n",
    "    return f'Abstract: {row[\"abstract\"]}\\nStatement: {row[\"statement\"]}\\nInstructions: {config.sup_instr}\\nScore: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sup = pd.read_csv(\"./data/Porpoise_2/train.tsv\", sep = \"\\t\")\n",
    "test_sup = pd.read_csv(\"./data/Porpoise_2/test.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sup[\"text\"] = train_sup.apply(lambda row: getText(row), axis = 1)\n",
    "train_sup[\"prompt\"] = train_sup.apply(lambda row: getPrompt(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sup[\"text\"] = test_sup.apply(lambda row: getText(row), axis = 1)\n",
    "test_sup[\"prompt\"] = test_sup.apply(lambda row: getPrompt(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pd.concat([train_sup[\"text\"], train_rel[\"text\"]], ignore_index = True)\n",
    "train_prompts = pd.concat([train_sup[\"prompt\"], train_rel[\"prompt\"]], ignore_index = True)\n",
    "\n",
    "test_text = pd.concat([test_sup[\"text\"], test_rel[\"text\"]], ignore_index = True)\n",
    "test_prompts = pd.concat([test_sup[\"prompt\"], test_rel[\"prompt\"]], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({\"text\": train_text, \"prompt\": train_prompts})\n",
    "test = pd.DataFrame({\"text\": test_text, \"prompt\": test_prompts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZEPpusOVNUQ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635,
     "referenced_widgets": [
      "ee7f42a1fc0e4e67bfb38456037a5df6",
      "cca30912cc6f45dabd4a8917b94e34fd",
      "d19d664486b4460ba8d71ca26fbcc35f",
      "48b28458b5844e6289a668125f6baaf1",
      "939ea3533459424aab71593aac6d76cc",
      "74ebdf4619fb46f980d875cce6d764ba",
      "908166e01f834c55a4edeb30045ce49c",
      "e5169fdcd78643819dfd415087779161"
     ]
    },
    "id": "85-066iG0MU8",
    "outputId": "f4965b9b-b6b5-4a96-874b-9192db9891b8"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"kmGPT\", entity = \"morgridge\", group = \"Porpoise 2.0\", name = \"Dora?\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlSge3EMgTes"
   },
   "outputs": [],
   "source": [
    "from transformers.integrations import WandbCallback\n",
    "class LLMSampleCB(WandbCallback):\n",
    "    def __init__(self, trainer, test_sup, test_rel):\n",
    "        super().__init__()\n",
    "        self.test_rel = test_rel\n",
    "        self.test_sup = test_sup\n",
    "        \n",
    "        self.y_sup = torch.tensor(self.test_sup[\"label\"])\n",
    "        self.y_rel = torch.tensor(self.test_rel[\"label\"])\n",
    "        \n",
    "        self.model, self.tokenizer = trainer.model, trainer.tokenizer\n",
    "\n",
    "    def get_metrics(self, test_set, labels):\n",
    "        FastLanguageModel.for_inference(trainer.model)\n",
    "        y_hat = []\n",
    "        for i in tqdm(range(len(test_set[\"prompt\"]))):\n",
    "            prompt = test_set[\"prompt\"][i]\n",
    "            prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "            out = trainer.model.generate(prompt_ids.cuda(), max_new_tokens = 1)[-1]\n",
    "            response = tokenizer.decode(out)\n",
    "            try:\n",
    "                score = int(response[-1])\n",
    "            except:\n",
    "                score = 1 - labels[i]\n",
    "            y_hat.append(score)\n",
    "\n",
    "        y_hat = torch.tensor(y_hat)\n",
    "\n",
    "        acc = accuracy_score(labels, y_hat)\n",
    "        prec = precision_score(labels, y_hat, average='weighted')\n",
    "        recall = recall_score(labels, y_hat, average='weighted')\n",
    "        f1 = f1_score(labels, y_hat, average='weighted')\n",
    "\n",
    "        return acc, prec, recall, f1\n",
    "\n",
    "    def on_evaluate(self, args, state, control,  **kwargs):\n",
    "        super().on_evaluate(args, state, control, **kwargs)\n",
    "        acc, prec, recall, f1 = self.get_metrics(self.test_rel, self.y_rel)\n",
    "        self._wandb.log({\"Relevance Running Validation Accuracy\": acc})\n",
    "        self._wandb.log({\"Relevance Running Validation Precision\": prec})\n",
    "        self._wandb.log({\"Relevance Running Validation Recall\": recall})\n",
    "        self._wandb.log({\"Relevance Running Validation F1\": f1})\n",
    "        epoch = math.ceil(trainer.state.epoch)\n",
    "        print(\"*********** RELEVANCE FILTERING RESULTS ***********\")\n",
    "        print(f\"Epoch {epoch}:\\n\\tAccuracy: {acc:.3f}\\n\\tPrecision: {prec:.3f}\\n\\tRecall: {recall:.3f}\\n\\tF-1 Score: {f1:.3f}\")\n",
    "\n",
    "        acc, prec, recall, f1 = self.get_metrics(self.test_sup, self.y_sup)\n",
    "        self._wandb.log({\"Support Running Validation Accuracy\": acc})\n",
    "        self._wandb.log({\"Support Running Validation Precision\": prec})\n",
    "        self._wandb.log({\"Support Running Validation Recall\": recall})\n",
    "        self._wandb.log({\"Support Running Validation F1\": f1})\n",
    "        epoch = math.ceil(trainer.state.epoch)\n",
    "        print(\"*********** SUPPORT RESULTS ***********\")\n",
    "        print(f\"Epoch {epoch}:\\n\\tAccuracy: {acc:.3f}\\n\\tPrecision: {prec:.3f}\\n\\tRecall: {recall:.3f}\\n\\tF-1 Score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02m7dQxMT-gM"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"checkpoints\",\n",
    "    report_to = \"wandb\",\n",
    "    learning_rate = 2e-4,\n",
    "    warmup_ratio = 0.03,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    num_train_epochs = 15,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    bf16 = True,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_steps = 1,\n",
    "    do_eval=True,\n",
    "    neftune_noise_alpha = 5,\n",
    "    weight_decay = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YAr8WK7TWhj",
    "outputId": "b8d41ec8-04c7-4b05-b619-edc1c5d2cc65"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    args = training_args,\n",
    "    model=model,\n",
    "    # peft_config=peft_config,\n",
    "    # data_collator=DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer = tokenizer),\n",
    "    packing = True,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzRal5zAgTet"
   },
   "outputs": [],
   "source": [
    "wandb_callback = LLMSampleCB(trainer, test_sup, test_rel)\n",
    "trainer.add_callback(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "MPQibNsRT3Qz",
    "outputId": "8462bffe-6833-41bd-bb7a-cde55eefe6b9"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        prompt = test_sup[\"prompt\"][0]\n",
    "        prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        out = model.generate(prompt_ids.cuda(), max_new_tokens = 100)\n",
    "        response = tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xYmG8ex0tN2",
    "outputId": "f5349e88-12bb-418e-a5d9-de23e62b2406"
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        y_hat = []\n",
    "        cots = []\n",
    "        num_wrong = 0\n",
    "        for i in tqdm(range(len(test[\"prompt\"]))):\n",
    "            prompt = test[\"prompt\"][i]\n",
    "            prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "            out = trainer.model.generate(prompt_ids.cuda(), max_new_tokens = 1)\n",
    "            response = tokenizer.decode(out[0])\n",
    "            score = int(response[-1])\n",
    "            cot = \"Correct! So no explanation was given.\"\n",
    "\n",
    "            if score != test[\"label\"][i]:\n",
    "                rationale = trainer.model.generate(prompt_ids.cuda(), max_new_tokens = 400)\n",
    "                rationale = tokenizer.decode(rationale[0])\n",
    "                prompt, ans = rationale.split(\"Score: \")\n",
    "                cot = ans[1:]\n",
    "                num_wrong += 1\n",
    "                print(\"wrong\")\n",
    "            \n",
    "            y_hat.append(score)\n",
    "            cots.append(cot)\n",
    "            # print(score)\n",
    "print(num_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxJ3A6jNgTeu"
   },
   "outputs": [],
   "source": [
    "y = torch.tensor(test[\"label\"])\n",
    "y_hat = torch.tensor(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(test[\"prompt\"], y_hat, y, cots))\n",
    "test_table = wandb.Table(columns = [\"prompt\", \"y_hat\", \"y\", \"rationale\"], data = data)\n",
    "wandb.log({\"Predictions\": test_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "0HSYlwiEUFfg",
    "outputId": "0ff433f5-4f04-465b-97b3-92b4cfb2c866"
   },
   "outputs": [],
   "source": [
    "wandb.log({\"Validation Accuracy\": accuracy_score(y, y_hat)})\n",
    "wandb.log({\"Validation Precision\": precision_score(y, y_hat, average='weighted')})\n",
    "wandb.log({\"Validation Recall\": recall_score(y, y_hat, average='weighted')})\n",
    "wandb.log({\"Validation F1-Score\": f1_score(y, y_hat, average='weighted')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDQT2DoWgTeu",
    "outputId": "6347a539-f3b9-4344-fa98-b809c32fcfad"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5eNeh-BVGzY",
    "outputId": "6de3b098-89da-4e5c-e03c-5451bedb80c0"
   },
   "outputs": [],
   "source": [
    "precision_score(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EacGTrduVIit",
    "outputId": "a2bee6c2-c45f-4fbf-9216-cdb514c1776b"
   },
   "outputs": [],
   "source": [
    "recall_score(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIJPREInVJ9S",
    "outputId": "80311d01-7baf-44ca-ee0c-dcd57c58fa2b"
   },
   "outputs": [],
   "source": [
    "f1_score(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD9JouaxCTsk"
   },
   "outputs": [],
   "source": [
    "wandb.log({f\"Confusion Matrix\": wandb.plot.confusion_matrix(y_true=y.tolist(), preds=y_hat.tolist(), class_names=[\"Irrelevant\", \"Relevant\"], title = \"Relevance Confusion Matrix\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71xydTqqgTev"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5drLanWzgTev"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"Porpoise1\", tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\"hf/porpoise1\", tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = FastLanguageModel.from_pretrained(\"lexu14/porpoise1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        prompt = test[\"prompt\"][0]\n",
    "        prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        out = m.generate(prompt_ids.cuda(), max_new_tokens = 100)\n",
    "        response = tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "48b28458b5844e6289a668125f6baaf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74ebdf4619fb46f980d875cce6d764ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "908166e01f834c55a4edeb30045ce49c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "939ea3533459424aab71593aac6d76cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cca30912cc6f45dabd4a8917b94e34fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_939ea3533459424aab71593aac6d76cc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_74ebdf4619fb46f980d875cce6d764ba",
      "value": "0.035 MB of 0.035 MB uploaded\r"
     }
    },
    "d19d664486b4460ba8d71ca26fbcc35f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_908166e01f834c55a4edeb30045ce49c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5169fdcd78643819dfd415087779161",
      "value": 1
     }
    },
    "e5169fdcd78643819dfd415087779161": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ee7f42a1fc0e4e67bfb38456037a5df6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cca30912cc6f45dabd4a8917b94e34fd",
       "IPY_MODEL_d19d664486b4460ba8d71ca26fbcc35f"
      ],
      "layout": "IPY_MODEL_48b28458b5844e6289a668125f6baaf1"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
