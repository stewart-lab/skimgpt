{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "import json\n",
    "from Bio import Entrez\n",
    "# import vllm\n",
    "# from lmformatenforcer import RegexParser\n",
    "# from lmformatenforcer.integrations.vllm import build_vllm_logits_processor, build_vllm_token_enforcer_tokenizer_data\n",
    "import argparse\n",
    "from abstract_comprehension import read_tsv_to_dataframe\n",
    "from utils import Config, RaggedTensor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import jinja2\n",
    "from classifier import process_single_row, write_to_json, test_openai_connection\n",
    "        \n",
    "# Returns either AB or BC hypotheses depending on the input. If A, B is passed in, getHypothesis will retrieve the AB hypothesis. \n",
    "# Only two arguements should be specified at once.\n",
    "def getHypothesis(config, a_term: str = None, b_term: str = None, c_term: str = None) -> str:\n",
    "    job_type = config.get(\"JOB_TYPE\", \"\").lower()\n",
    "    \n",
    "    if job_type == \"km_with_gpt\":\n",
    "        assert a_term and b_term and not c_term\n",
    "        hypothesis_template = config.get(\"KM_hypothesis\", \"\")\n",
    "        \n",
    "        return hypothesis_template.format(a_term=a_term, b_term=b_term)\n",
    "    \n",
    "    elif job_type == \"position_km_with_gpt\":\n",
    "        assert a_term and b_term and not c_term\n",
    "        \n",
    "        hypothesis_template = config.get(\"POSITION_KM_hypothesis\", \"\")\n",
    "        return hypothesis_template.format(a_term=a_term, b_term=b_term), None\n",
    "    \n",
    "    elif job_type == \"skim_with_gpt\":\n",
    "        assert (a_term and b_term and not c_term) or (b_term and c_term and not a_term)\n",
    "        \n",
    "        if a_term and b_term and not c_term:\n",
    "            hypothesis_template = config.get(\"SKIM_hypotheses\", \"\").get(\"AB\")\n",
    "            return hypothesis_template.format(a_term=a_term, b_term=b_term)\n",
    "        \n",
    "        elif b_term and c_term and not a_term:\n",
    "            hypothesis_template = config.get(\"SKIM_hypotheses\", \"\").get(\"BC\")\n",
    "            return hypothesis_template.format(b_term=b_term, c_term = c_term)\n",
    "\n",
    "    else:\n",
    "        return \"No valid hypothesis for the provided JOB_TYPE.\"\n",
    "    \n",
    "    \n",
    "\n",
    "def cot_prompt(sys_prompt: str, hyp: str, abstract: str) -> str:\n",
    "    context = {\n",
    "        \"sys_prompt\": sys_prompt,\n",
    "        \"hyp\": hyp,\n",
    "        \"abstract\": abstract,\n",
    "    }\n",
    "    \n",
    "    template = jinja2.Template(\"\"\"\n",
    "        <|im_start|>system\n",
    "        {{sys_prompt}}\n",
    "        <|im_end|>\n",
    "        <|im_start|>user\n",
    "        Hypothesis: {{hyp}}\n",
    "        Abstract: {{abstract}}\n",
    "\n",
    "        Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "        Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Make sure to stay focused on what the hypothesis is specifically saying. Let's work this out in a step by step way to be sure we have the right answer.\n",
    "        <|im_end|>\n",
    "        <|im_start|>assistant                           \n",
    "    \"\"\")\n",
    "    \n",
    "    return template.render(context)\n",
    "\n",
    "def answer_prompt(sys_prompt: str, hyp: str, abstract: str, chain_of_thought: str, continuous: bool) -> str:\n",
    "    context = {\n",
    "        \"sys_prompt\": sys_prompt,\n",
    "        \"hyp\": hyp,\n",
    "        \"abstract\": abstract,\n",
    "        \"chain_of_thought\": chain_of_thought,\n",
    "        \"continuous\": continuous\n",
    "    }\n",
    "    \n",
    "    template = jinja2.Template(\"\"\"\n",
    "        <|im_start|>system\n",
    "        {{sys_prompt}}\n",
    "        <|im_end|>\n",
    "        <|im_start|>user\n",
    "        Hypothesis: {{hyp}}\n",
    "        Abstract: {{abstract}}\n",
    "\n",
    "        Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "        Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Make sure to stay focused on what the hypothesis is specifically saying. Let's work this out in a step by step way to be sure we have the right answer.\n",
    "        {{chain_of_thought}}\n",
    "        \n",
    "        {% if continuous %}\n",
    "        Classify the given abstract with a score between 0 (Not relevant for scientifically assessing the hypothesis) and 1 (Relevant for scientifically assessing the hypothesis) based on the reasoning above and other useful pieces of information in the abstract and hypothesis.\n",
    "        {% else %}\n",
    "        Classify the given abstract as either 0 (Not relevant for scientifically assessing the hypothesis) or 1 (Relevant for scientifically assessing the hypothesis) based on the reasoning above and other useful pieces of information in the abstract and hypothesis.\n",
    "        {% endif %}\n",
    "        Answer: \n",
    "        <|im_end|>\n",
    "        <|im_start|>assistant\n",
    "    \"\"\")\n",
    "    \n",
    "    return template.render(context)\n",
    "    \n",
    "# def gen(prompts: RaggedTensor, model: any, sampling_config: vllm.SamplingParams) -> RaggedTensor:\n",
    "# \tgenerated = model.generate(prompts.data, sampling_params = sampling_config)\n",
    "# \toutputs = RaggedTensor([output.outputs[0].text for output in generated], prompts.break_point)\n",
    "# \treturn outputs\n",
    "\n",
    "def getCoTPrompts(abstracts: RaggedTensor, sys_prompt: str, hypotheses: RaggedTensor) -> RaggedTensor:\n",
    "    assert not abstracts.is2D(), \"abstracts should be flattened.\"\n",
    "    assert not hypotheses.is2D(), \"hypotheses should be flattened.\"\n",
    "    return RaggedTensor([cot_prompt(sys_prompt, hypotheses[i], abstracts[i]) for i in range(abstracts.shape)], hypotheses.break_point)\n",
    "\n",
    "def getAnswerPrompts(abstracts: RaggedTensor, sys_prompt: str, hypotheses: RaggedTensor, cot_outputs: RaggedTensor, continuous: bool) -> RaggedTensor:\n",
    "    assert not abstracts.is2D(), \"abstracts should be flattened.\"\n",
    "    assert not hypotheses.is2D(), \"hypotheses should be flattened.\"\n",
    "    assert not cot_outputs.is2D(), \"cot outputs should be flattened\"\n",
    "    return RaggedTensor([answer_prompt(sys_prompt, hypotheses[i], abstracts[i], cot_outputs[i], continuous) for i in range(abstracts.shape)], hypotheses.break_point)\n",
    "    \n",
    "# Returns a dictionary for each PMID & Abstract Pair\n",
    "# This method is needed since Entrez automatically removes duplicates in the pmid list\n",
    "def getAbstractMap(config: json, pmids: list[str]) -> dict:\n",
    "    returned_pmids = []\n",
    "    returned_abstracts = []\n",
    "    global_config = config[\"GLOBAL_SETTINGS\"]\n",
    "    pmid_config = global_config[\"PUBMED_PARAMS\"]\n",
    "    \n",
    "    Entrez.email = 'leoxu27@gmail.com'\n",
    "    Entrez.api_key = \"8bfe67116f93cedbee9e4f31a1e65b7e1d09\"\n",
    "    Entrez.max_tries = global_config[\"MAX_RETRIES\"]\n",
    "    Entrez.sleep_between_tries = global_config[\"RETRY_DELAY\"]\n",
    "    efetch = Entrez.efetch(db=pmid_config[\"db\"], id=pmids, rettype=pmid_config[\"rettype\"])\n",
    "    \n",
    "    output = Entrez.read(efetch)\n",
    "    efetch.close()\n",
    "    \n",
    "    for paper in output[\"PubmedArticle\"]:\n",
    "        returned_pmids.append(str(paper[\"MedlineCitation\"][\"PMID\"]))\n",
    "        abstract_text = \" \".join(paper[\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"])\n",
    "        returned_abstracts.append(abstract_text)\n",
    "    return dict(zip(returned_pmids, returned_abstracts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job type detected. Running skim_with_gpt.\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='Mistral7B Inference')\n",
    "\n",
    "# parser.add_argument('--km_output', type=str, required=True, help='Path to the TSV file holding a km run output.')\n",
    "# parser.add_argument('--config', type=str, required=True, help='Config file for kmGPT run.')\n",
    "# args = parser.parse_args()\n",
    "class args:\n",
    "    km_output = \"../output/output_20240320124502/skim_with_gpt_Diabetes_Fish oil_output.tsv\"\n",
    "    config = \"../config.json\"\n",
    "###################### AB Data Loading & Processsing ############################ \n",
    "config = Config(args)\n",
    "cot_df = config.data.copy(deep = True)\n",
    "filtered_df = config.data.copy(deep = True)\n",
    "\n",
    "a_term = config.data.a_term.unique().tolist()[0].split(\"&\")[0]\n",
    "b_terms = config.data.b_term.unique().tolist()\n",
    "\n",
    "ab_pmids = RaggedTensor([eval(lst) for lst in config.data.ab_pmid_intersection])\n",
    "ab_hypotheses = RaggedTensor([getHypothesis(config.job_config, a_term = a_term, b_term = b_term) for b_term in b_terms])\n",
    "\n",
    "all_pmids = ab_pmids.flatten()\n",
    "all_hypotheses = ab_hypotheses.expand(ab_pmids.shape)\n",
    "###################### BC Data Loading & Processsing ############################ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PUBMED_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \tall_pmids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bc_pmids\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      7\u001b[0m \tall_hypotheses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bc_hypotheses\u001b[38;5;241m.\u001b[39mexpand(bc_pmids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m abstract_map \u001b[38;5;241m=\u001b[39m \u001b[43mgetAbstractMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_pmids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m abstracts \u001b[38;5;241m=\u001b[39m all_pmids\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m pmid: abstract_map\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mstr\u001b[39m(pmid), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 134\u001b[0m, in \u001b[0;36mgetAbstractMap\u001b[0;34m(config, pmids)\u001b[0m\n\u001b[1;32m    131\u001b[0m pmid_config \u001b[38;5;241m=\u001b[39m global_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUBMED_PARAMS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m Entrez\u001b[38;5;241m.\u001b[39memail \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleoxu27@gmail.com\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 134\u001b[0m Entrez\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUBMED_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    135\u001b[0m Entrez\u001b[38;5;241m.\u001b[39mmax_tries \u001b[38;5;241m=\u001b[39m global_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX_RETRIES\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    136\u001b[0m Entrez\u001b[38;5;241m.\u001b[39msleep_between_tries \u001b[38;5;241m=\u001b[39m global_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETRY_DELAY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PUBMED_API_KEY'"
     ]
    }
   ],
   "source": [
    "if config.is_skim_gpt:\n",
    "\tc_term = config.data.c_term.unique().tolist()[0]\n",
    "\tbc_pmids = RaggedTensor([eval(lst) for lst in config.data.bc_pmid_intersection])\n",
    "\tbc_hypotheses = RaggedTensor([getHypothesis(config.job_config, c_term = c_term, b_term = b_term) for b_term in b_terms])\n",
    "\n",
    "\tall_pmids += bc_pmids.flatten()\n",
    "\tall_hypotheses += bc_hypotheses.expand(bc_pmids.shape)\n",
    "\t\n",
    "abstract_map = getAbstractMap(config.job_config, all_pmids)\n",
    "abstracts = all_pmids.map(lambda pmid: abstract_map.get(str(pmid), \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################### LLM Inference ############################\n",
    "cot_prompts = getCoTPrompts(abstracts, config.sys_prompt, all_hypotheses)\n",
    "cot_outputs = gen(cot_prompts, mistral, sampling_cot)\n",
    "\n",
    "answer_prompts = getAnswerPrompts(abstracts, config.sys_prompt, all_hypotheses, cot_outputs, config.continuous)\n",
    "answers = gen(answer_prompts, mistral, sampling_answer)\n",
    "answers = answers.map(lambda x: eval(x))\n",
    "\n",
    "##################### Post process AB answers ############################\n",
    "answer_mask = answers\n",
    "if config.continuous:\n",
    "\tanswer_mask = answer_mask.getFullKArgMax(k = config.k)\n",
    "\tprint(answer_mask.shape)\n",
    "\n",
    "ab_answer_masks, bc_answer_masks = answer_mask.split()\n",
    "ab_raw_scores, bc_raw_scores = answers.split()\n",
    "ab_abstracts, bc_abstracts = abstracts.split()\n",
    "ab_cot, bc_cot = cot_outputs.split()\n",
    "\n",
    "ab_answer_masks.reshape(ab_pmids.shape)\n",
    "ab_raw_scores.reshape(ab_pmids.shape)\n",
    "ab_abstracts.reshape(ab_pmids.shape)\n",
    "ab_abstracts.applyFilter(ab_answer_masks)\n",
    "ab_cot.reshape(ab_pmids.shape)\n",
    "\n",
    "filtered_df[\"ab_pmid_intersection\"] = ab_abstracts.data\n",
    "cot_df[\"ab_mask\"] = ab_answer_masks.data\n",
    "cot_df[\"ab_score\"] = ab_raw_scores.data\n",
    "cot_df[\"ab_cot\"] = ab_cot.data\n",
    "cot_df[\"ab_hypothesis\"] = ab_hypotheses.data\n",
    "\n",
    "##################### Post process BC answers ############################ \n",
    "if config.is_skim_gpt:\n",
    "\tbc_answer_masks.reshape(bc_pmids.shape)\n",
    "\tbc_raw_scores.reshape(bc_pmids.shape)\n",
    "\tbc_abstracts.reshape(bc_pmids.shape)\n",
    "\tbc_abstracts.applyFilter(bc_answer_masks)\n",
    "\tbc_cot.reshape(bc_pmids.shape)\n",
    "\n",
    "\tfiltered_df[\"bc_pmid_intersection\"] = bc_abstracts.data\n",
    "\tcot_df[\"bc_mask\"] = bc_answer_masks.data\n",
    "\tcot_df[\"bc_score\"] = bc_raw_scores.data\n",
    "\tcot_df[\"bc_cot\"] = bc_cot.data\n",
    "\tcot_df[\"bc_hypothesis\"] = bc_hypotheses.data\n",
    "\n",
    "filtered_df.to_csv(f\"{config.filtered_tsv_name}\", sep=\"\\t\")\n",
    "cot_df.to_csv(f\"{config.cot_tsv_name}\", sep=\"\\t\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
