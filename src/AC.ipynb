{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "import json\n",
    "from Bio import Entrez\n",
    "# import vllm\n",
    "# from lmformatenforcer import RegexParser\n",
    "# from lmformatenforcer.integrations.vllm import build_vllm_logits_processor, build_vllm_token_enforcer_tokenizer_data\n",
    "import argparse\n",
    "from abstract_comprehension import read_tsv_to_dataframe\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import jinja2\n",
    "from classifier import process_single_row, write_to_json, test_openai_connection\n",
    "from itertools import chain\n",
    "\n",
    "class RaggedTensor:\n",
    "\tdef __init__(self, data, break_point = []):\n",
    "\t\tself.data = data\n",
    "\t\tself.break_point = break_point\n",
    "\t\tself.index = 0\n",
    "\t\tself.getShape()\n",
    "\t\t\n",
    "\tdef getShape(self) -> None:\n",
    "\t\tif self.is2D():\n",
    "\t\t\tself.shape = [len(i) for i in self.data]\n",
    "\t\telse:\n",
    "\t\t\tself.shape = len(self.data)\n",
    "\t\t\t\n",
    "\tdef is2D(self) -> bool:\n",
    "\t\tif not(len(self.data) == 0):\n",
    "\t\t\treturn isinstance(self.data[0], list)\n",
    "\t\telse:\n",
    "\t\t\treturn False;\n",
    "\t\n",
    "\t# Duplicates each element in data according to the shape_list\n",
    "\tdef expand(self, shape_list: list) -> None:\n",
    "\t\tassert not self.is2D(), \"Data must be 1D before calling expand. Call flatten first?\"\n",
    "\t\tassert self.shape == len(shape_list), \"The length of shape list must equal the length of data\"\n",
    "\t\t\n",
    "\t\texpanded = []\n",
    "\t\tfor idx, inp in enumerate(self.data):\n",
    "\t\t\texpanded.extend([inp] * shape_list[idx])\n",
    "\t\t\t\n",
    "\t\treturn RaggedTensor(expanded)\n",
    "\t\t\n",
    "\tdef flatten(self) -> RaggedTensor:\n",
    "\t\tif self.is2D():\n",
    "\t\t\toutput = []\n",
    "\t\t\tfor lst in self.data:\n",
    "\t\t\t\toutput.extend(lst)\n",
    "\t\t\treturn RaggedTensor(output)\n",
    "\t\telse:\n",
    "\t\t\treturn self\n",
    "\t\t\n",
    "\t# Inverts the expand method\n",
    "\tdef compress(self, shape_list: list):\n",
    "\t\tassert self.shape == sum(shape_list)\n",
    "\t\tself.data = list(set(self.data))\n",
    "\t\tself.getShape()\n",
    "\t\n",
    "\t# Splits the data depending on the index\n",
    "\tdef split(self) -> list[RaggedTensor]:\n",
    "\t\tif len(self.break_point) == 0:\n",
    "\t\t\tprint(\"Warning: No breakpoint was specified.\")\n",
    "\t\t\treturn self, RaggedTensor([])\n",
    "\t\tpast_break_point = 0\n",
    "\t\toutput = []\n",
    "\t\tfor break_point in self.break_point:\n",
    "\t\t\toutput.append(RaggedTensor(self.data[past_break_point:break_point]))\n",
    "\t\t\tpast_break_point = break_point\n",
    "\t\toutput.append(RaggedTensor(self.data[past_break_point:]))\n",
    "\t\treturn output\n",
    "\t\n",
    "\t# Reshapes the data depending on the input\n",
    "\tdef reshape(self, shape: list) -> list:\n",
    "\t\tassert not self.is2D(), \"Reshape only works with 1D tensors.\"\n",
    "\t\tassert self.shape == sum(shape), \"The shape of the tensor should be equal to the sum of the wanted shape.\"\n",
    "\t\toutput = []\n",
    "\t\trunning_length = 0;\n",
    "\t\tfor length in shape:\n",
    "\t\t\toutput.append(self.data[running_length: running_length + length])\n",
    "\t\t\trunning_length += length\n",
    "\t\t\t\n",
    "\t\tself.data = output\n",
    "\t\tself.getShape()\n",
    "\t\n",
    "\t# Applies a mask to the tensor\n",
    "\tdef applyFilter(self, mask: RaggedTensor) -> None:\n",
    "\t\tassert self.shape == mask.shape, \"Filtering only works when the shapes are the same\"\n",
    "\t\tif self.is2D():\n",
    "\t\t\tfor i in range(len(self.data)):\n",
    "\t\t\t\tboolean_mask = np.array(mask[i]) == 1\n",
    "\t\t\t\tself.data[i] = list(np.array(self.data[i])[boolean_mask])\n",
    "\t\telse:\n",
    "\t\t\tboolean_mask = np.array(mask) == 1\n",
    "\t\t\tself.data = list(np.array(self.data)[boolean_mask])\n",
    "\t\t\n",
    "\t# Applies a function to the tensor\n",
    "\tdef map(self, func: callable, *args) -> RaggedTensor:\n",
    "\t\tassert not self.is2D(), \"Map only works with 1D tensors\"\n",
    "\t\treturn RaggedTensor([func(i, *args) for i in self.data], self.break_point)\n",
    "\t\n",
    "\t# Helper method to mask the top values in data\n",
    "\tdef getTopKMask(self, array, k: int) -> list:\n",
    "\t\ttop_k_indices = np.argsort(array, axis=None)[-k:]\n",
    "\n",
    "\t\t# Create a binary mask with the top 5 values set to True.\n",
    "\t\tmask = np.zeros_like(array, dtype=bool)\n",
    "\t\tmask[top_k_indices] = True\n",
    "\t\treturn mask.tolist()\n",
    "\t\n",
    "\t# Creates a mask of the data with the top k values in each row of the data\n",
    "\tdef getFullKArgMax(self, k: int) -> RaggedTensor:\n",
    "\t\toutput = []\n",
    "\t\tif self.is2D():\n",
    "\t\t\tfor array in self.data:\n",
    "\t\t\t\tmask = self.getTopKMask(array, k)\n",
    "\t\t\t\toutput.append(mask)\n",
    "\t\telse:\n",
    "\t\t\toutput = self.getTopKMask(self.data, k)\n",
    "\t\t\t\n",
    "\t\toutput = RaggedTensor(output, self.break_point)\n",
    "\t\tassert output.shape == self.shape\n",
    "\t\t\n",
    "\t\treturn output\n",
    "\t\t\t\n",
    "\t\t\n",
    "\tdef __add__(self, other: RaggedTensor) -> RaggedTensor:\n",
    "\t\tassert not self.is2D(), \"Adding only works with flattened tensors\"\n",
    "\t\tbreak_point = self.shape\n",
    "\t\treturn RaggedTensor(self.data + other.data, self.break_point + [break_point])\n",
    "\t\n",
    "\tdef __str__(self):\n",
    "\t\treturn str(self.data)\n",
    "\t\n",
    "\tdef __iter__(self):\n",
    "\t\treturn self.flatten().data.__iter__()\n",
    "\t\n",
    "\tdef __getitem__(self, index: int) -> any:\n",
    "\t\treturn self.data[index]\n",
    "\t\n",
    "class Config:\n",
    "\tdef __init__(self, args: dict):\n",
    "\t\tself.data = read_tsv_to_dataframe(args.km_output)\n",
    "\t\twith open(args.config, 'r') as config_file:\n",
    "\t\t\tself.job_config = json.load(config_file)\n",
    "\t\t\n",
    "\t\tself.global_settings = self.job_config[\"GLOBAL_SETTINGS\"]\n",
    "\t\tself.k = self.global_settings[\"MAX_ABSTRACTS\"]\n",
    "\t\tassert self.k > 0\n",
    "\t\tself.km_output_dir = os.path.dirname(args.km_output)\n",
    "\t\tself.km_output_base_name = os.path.splitext(os.path.basename(args.km_output))[0]\n",
    "\n",
    "\t\t# Ensure the directory exists\n",
    "\t\tif not os.path.exists(self.km_output_dir) and self.km_output_dir != '':\n",
    "\t\t\tos.makedirs(self.km_output_dir)\n",
    "\t\t\n",
    "\t\tself.filtered_tsv_name = os.path.join(self.km_output_dir, f\"filtered_{self.km_output_base_name}.tsv\")\n",
    "\t\tself.cot_tsv_name = os.path.join(self.km_output_dir, f\"cot_{self.km_output_base_name}.tsv\")\n",
    "\t\tself.job_type = self.job_config.get('JOB_TYPE')\n",
    "\t\tself.filter_config = self.job_config[\"abstract_filter\"]\n",
    "\t\t\n",
    "\t\tself.sys_prompt = self.filter_config['SYS_PROMPT']\n",
    "\t\tself.is_skim_gpt = self.job_type == \"skim_with_gpt\"\n",
    "\t\tself.has_ac = \"ac_pmid_intersection\" in self.data.columns\n",
    "\t\tself.continuous = self.filter_config[\"CONTINUOUS_SCORE\"]\n",
    "\t\t\n",
    "\t\tself.regex = r'[0][.]\\d{5}' if self.continuous else r'0|1'\n",
    "\t\tself.max_score_tokens = 7 if self.continuous else 1\n",
    "\t\tself.max_cot_tokens = self.filter_config[\"MAX_COT_TOKENS\"]\n",
    "\t\t\n",
    "\t\tprint(f\"Job type detected. Running {self.job_type}.\")\n",
    "\t\tif self.is_skim_gpt:\n",
    "\t\t\tassert \"c_term\" in self.data.columns, \"Input TSV must have c_term if running skim_with_gpt\"\n",
    "\t\t\tassert \"bc_pmid_intersection\" in self.data.columns, \"Input TSV must have an bc_pmid_intersection.\"\n",
    "\t\t\n",
    "\t\tassert \"ab_pmid_intersection\" in self.data.columns, \"Input TSV must have an ab_pmid_intersection.\"\n",
    "\t\tassert \"a_term\" in self.data.columns, \"Input TSV must have an a_term.\"\n",
    "\t\tassert \"b_term\" in self.data.columns, \"Input TSV must have a b_term\"\n",
    "\t\t\n",
    "# Returns either AB or BC hypotheses depending on the input. If A, B is passed in, getHypothesis will retrieve the AB hypothesis. \n",
    "# Only two arguements should be specified at once.\n",
    "def getHypothesis(config, a_term: str = None, b_term: str = None, c_term: str = None) -> str:\n",
    "\tjob_type = config.get(\"JOB_TYPE\", \"\").lower()\n",
    "\t\n",
    "\tif job_type == \"km_with_gpt\":\n",
    "\t\tassert a_term and b_term and not c_term\n",
    "\t\thypothesis_template = config.get(\"KM_hypothesis\", \"\")\n",
    "\t\t\n",
    "\t\treturn hypothesis_template.format(a_term=a_term, b_term=b_term)\n",
    "\t\n",
    "\telif job_type == \"position_km_with_gpt\":\n",
    "\t\tassert a_term and b_term and not c_term\n",
    "\t\t\n",
    "\t\thypothesis_template = config.get(\"POSITION_KM_hypothesis\", \"\")\n",
    "\t\treturn hypothesis_template.format(a_term=a_term, b_term=b_term), None\n",
    "\t\n",
    "\telif job_type == \"skim_with_gpt\":\n",
    "\t\tassert (a_term and b_term and not c_term) or (b_term and c_term and not a_term) or (a_term and c_term and not b_term)\n",
    "\t\t\n",
    "\t\tif a_term and b_term and not c_term:\n",
    "\t\t\thypothesis_template = config.get(\"SKIM_hypotheses\", \"\").get(\"AB\")\n",
    "\t\t\treturn hypothesis_template.format(a_term=a_term, b_term=b_term)\n",
    "\t\t\n",
    "\t\telif b_term and c_term and not a_term:\n",
    "\t\t\thypothesis_template = config.get(\"SKIM_hypotheses\", \"\").get(\"BC\")\n",
    "\t\t\treturn hypothesis_template.format(b_term=b_term, c_term = c_term)\n",
    "\n",
    "\t\telif a_term and c_term and not b_term:\n",
    "\t\t\thypothesis_template = config.get(\"SKIM_hypotheses\", \"\").get(\"AC\")\n",
    "\t\t\treturn hypothesis_template.format(a_term=a_term, c_term = c_term)\n",
    "\n",
    "\telse:\n",
    "\t\treturn \"No valid hypothesis for the provided JOB_TYPE.\"\n",
    "\t\n",
    "\t\n",
    "\n",
    "def cot_prompt(sys_prompt: str, hyp: str, abstract: str) -> str:\n",
    "\tcontext = {\n",
    "\t\t\"sys_prompt\": sys_prompt,\n",
    "\t\t\"hyp\": hyp,\n",
    "\t\t\"abstract\": abstract,\n",
    "\t}\n",
    "\t\n",
    "\ttemplate = jinja2.Template(\"\"\"\n",
    "\t\t<|im_start|>system\n",
    "\t\t{{sys_prompt}}\n",
    "\t\t<|im_end|>\n",
    "\t\t<|im_start|>user\n",
    "\t\tHypothesis: {{hyp}}\n",
    "\t\tAbstract: {{abstract}}\n",
    "\n",
    "\t\tDetermine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "\t\tAnalyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Make sure to stay focused on what the hypothesis is specifically saying. Ignore redacted terms and make sure to look at the terms provided. Let's work this out in a step by step way to be sure we have the right answer. As a first step, use context clues to figure out the meaning of the terms given.\n",
    "\t\t<|im_end|>\n",
    "\t\t<|im_start|>assistant                           \n",
    "\t\"\"\")\n",
    "\t\n",
    "\treturn template.render(context)\n",
    "\n",
    "def answer_prompt(sys_prompt: str, hyp: str, abstract: str, chain_of_thought: str, continuous: bool) -> str:\n",
    "\tcontext = {\n",
    "\t\t\"sys_prompt\": sys_prompt,\n",
    "\t\t\"hyp\": hyp,\n",
    "\t\t\"abstract\": abstract,\n",
    "\t\t\"chain_of_thought\": chain_of_thought,\n",
    "\t\t\"continuous\": continuous\n",
    "\t}\n",
    "\t\n",
    "\ttemplate = jinja2.Template(\"\"\"\n",
    "\t\t<|im_start|>system\n",
    "\t\t{{sys_prompt}}\n",
    "\t\t<|im_end|>\n",
    "\t\t<|im_start|>user\n",
    "\t\tHypothesis: {{hyp}}\n",
    "\t\tAbstract: {{abstract}}\n",
    "\n",
    "\t\tDetermine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "\t\tAnalyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Make sure to stay focused on what the hypothesis is specifically saying. Ignore redacted terms and make sure to look at the terms provided. Let's work this out in a step by step way to be sure we have the right answer. As a first step, use context clues to figure out the meaning of the terms given.\n",
    "\t\t{{chain_of_thought}}\n",
    "\t\t\n",
    "\t\t{% if continuous %}\n",
    "\t\tClassify the given abstract with a score between 0 (Not relevant for scientifically assessing the hypothesis) and 1 (Relevant for scientifically assessing the hypothesis) based on the reasoning above and other useful pieces of information in the abstract and hypothesis.\n",
    "\t\t{% else %}\n",
    "\t\tClassify the given abstract as either 0 (Not relevant for scientifically assessing the hypothesis) or 1 (Relevant for scientifically assessing the hypothesis) based on the reasoning above and other useful pieces of information in the abstract and hypothesis.\n",
    "\t\t{% endif %}\n",
    "\t\tAnswer: \n",
    "\t\t<|im_end|>\n",
    "\t\t<|im_start|>assistant\n",
    "\t\"\"\")\n",
    "\t\n",
    "\treturn template.render(context)\n",
    "\t\n",
    "# def gen(prompts: RaggedTensor, model: any, sampling_config: vllm.SamplingParams) -> RaggedTensor:\n",
    "# \tgenerated = model.generate(prompts.data, sampling_params = sampling_config)\n",
    "# \toutputs = RaggedTensor([output.outputs[0].text for output in generated], prompts.break_point)\n",
    "# \treturn outputs\n",
    "\n",
    "def getCoTPrompts(abstracts: RaggedTensor, sys_prompt: str, hypotheses: RaggedTensor) -> RaggedTensor:\n",
    "\tassert not abstracts.is2D(), \"abstracts should be flattened.\"\n",
    "\tassert not hypotheses.is2D(), \"hypotheses should be flattened.\"\n",
    "\treturn RaggedTensor([cot_prompt(sys_prompt, hypotheses[i], abstracts[i]) for i in range(abstracts.shape)], hypotheses.break_point)\n",
    "\n",
    "def getAnswerPrompts(abstracts: RaggedTensor, sys_prompt: str, hypotheses: RaggedTensor, cot_outputs: RaggedTensor, continuous: bool) -> RaggedTensor:\n",
    "\tassert not abstracts.is2D(), \"abstracts should be flattened.\"\n",
    "\tassert not hypotheses.is2D(), \"hypotheses should be flattened.\"\n",
    "\tassert not cot_outputs.is2D(), \"cot outputs should be flattened\"\n",
    "\treturn RaggedTensor([answer_prompt(sys_prompt, hypotheses[i], abstracts[i], cot_outputs[i], continuous) for i in range(abstracts.shape)], hypotheses.break_point)\n",
    "\t\n",
    "# Returns a dictionary for each PMID & Abstract Pair\n",
    "# This method is needed since Entrez automatically removes duplicates in the pmid list\n",
    "def getAbstractMap(config: json, pmids: list[str]) -> dict:\n",
    "\treturned_pmids = []\n",
    "\treturned_abstracts = []\n",
    "\tglobal_config = config[\"GLOBAL_SETTINGS\"]\n",
    "\tpmid_config = global_config[\"PUBMED_PARAMS\"]\n",
    "\t\n",
    "\tEntrez.email = 'leoxu27@gmail.com'\n",
    "\tEntrez.api_key = config[\"PUBMED_API_KEY\"]\n",
    "\tEntrez.max_tries = global_config[\"MAX_RETRIES\"]\n",
    "\tEntrez.sleep_between_tries = global_config[\"RETRY_DELAY\"]\n",
    "\tefetch = Entrez.efetch(db=pmid_config[\"db\"], id=pmids, rettype=pmid_config[\"rettype\"])\n",
    "\t\n",
    "\toutput = Entrez.read(efetch)\n",
    "\tefetch.close()\n",
    "\t\n",
    "\tfor paper in output[\"PubmedArticle\"]:\n",
    "\t\tpmid = paper[\"MedlineCitation\"][\"PMID\"]\n",
    "\t\treturned_pmids.append(str(pmid))\n",
    "\t\tabstract_text = f'PMID {pmid}: {\" \".join(paper[\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"])}'\n",
    "\t\treturned_abstracts.append(abstract_text)\n",
    "\treturn dict(zip(returned_pmids, returned_abstracts))\n",
    "\n",
    "def postProcess(abstracts: RaggedTensor, cot: RaggedTensor, hypotheses: RaggedTensor, cot_df: pd.DataFrame, filtered_df: pd.DataFrame, terms: str, shape: list):\n",
    "\tabstracts.reshape(shape)\n",
    "\tcot.reshape(shape)\n",
    "\t\n",
    "\t# This is needed because there will only be one AC abstract list per TSV\n",
    "\tif terms == \"ac\":\n",
    "\t\tfiltered_df[f\"{terms}_pmid_intersection\"] = abstracts.data * len(filtered_df)\n",
    "\t\tcot_df[f\"{terms}_cot\"] = cot.data * len(filtered_df)\n",
    "\t\tcot_df[f\"{terms}_hypothesis\"] = hypotheses.data * len(filtered_df)\n",
    "  \n",
    "\telse:\n",
    "\t\tfiltered_df[f\"{terms}_pmid_intersection\"] = abstracts.data\n",
    "\t\tcot_df[f\"{terms}_cot\"] = cot.data\n",
    "\t\tcot_df[f\"{terms}_hypothesis\"] = hypotheses.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    km_output = \"../test_tsvs/skim_no_ac/skim_no_ac.tsv\"\n",
    "    config = \"../config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job type detected. Running skim_with_gpt.\n"
     ]
    }
   ],
   "source": [
    "config = Config(args)\n",
    "cot_df = config.data.copy(deep = True)\n",
    "filtered_df = config.data.copy(deep = True)\n",
    "\n",
    "a_term = config.data.a_term.unique().tolist()[0].split(\"&\")[0]\n",
    "b_terms = config.data.b_term.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_pmids = RaggedTensor([eval(lst) for lst in config.data.ab_pmid_intersection])\n",
    "ab_hypotheses = RaggedTensor([getHypothesis(config.job_config, a_term = a_term, b_term = b_term) for b_term in b_terms])\n",
    "\n",
    "all_pmids = ab_pmids.flatten()\n",
    "all_hypotheses = ab_hypotheses.expand(ab_pmids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.is_skim_gpt:\n",
    "\tc_term = config.data.c_term.unique().tolist()[0]\n",
    "\tbc_pmids = RaggedTensor([eval(lst) for lst in config.data.bc_pmid_intersection])\n",
    "\tbc_hypotheses = RaggedTensor([getHypothesis(config.job_config, c_term = c_term, b_term = b_term) for b_term in b_terms])\n",
    "\n",
    "\tall_pmids += bc_pmids.flatten()\n",
    "\tall_hypotheses += bc_hypotheses.expand(bc_pmids.shape)\n",
    "\t\n",
    "\tif config.has_ac:\n",
    "\t\t# For each atomic run there should only be one unique ac_pmid intersection\n",
    "\t\tac_pmids = RaggedTensor(eval(config.data.ac_pmid_intersection[0]))\n",
    "\t\tac_hypothesis = RaggedTensor([getHypothesis(config.job_config, a_term = a_term, c_term = c_term)])\n",
    "\t\tall_hypotheses += ac_hypothesis.expand([ac_pmids.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hypotheses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_map = getAbstractMap(config.job_config, all_pmids)\n",
    "abstracts = all_pmids.map(lambda pmid: abstract_map.get(str(pmid), \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 40]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts.break_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompts = getCoTPrompts(abstracts, config.sys_prompt, all_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 40]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot_prompts.break_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 40]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts.break_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = 3 * [RaggedTensor([])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_abstracts, bc_abstracts, ac_abstracts, *_ = chain(abstracts.split(), defaults)\n",
    "ab_cot, bc_cot, ac_cot, *_ = chain(cot_prompts.split(), defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "postProcess(ab_abstracts, ab_cot, ab_hypotheses, cot_df, filtered_df, terms = \"ab\", shape = ab_pmids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postProcess(config, ab_abstracts, ab_cot, ab_hypotheses, cot_df, filtered_df, terms = \"ab\", shape = ab_pmids.shape)\n",
    "if config.is_skim_gpt:\n",
    "\tpostProcess(config, bc_abstracts, bc_cot, ab_hypotheses, cot_df, filtered_df, terms = \"bc\", shape = bc_pmids.shape)\n",
    "\tif config.has_ac:\n",
    "\t\tpostProcess(config, ac_abstracts, ac_cot, ac_hypothesis, cot_df, filtered_df, terms = \"ac\", shape = [ac_pmids.shape])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
