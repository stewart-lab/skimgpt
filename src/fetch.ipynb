{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_pubmed_text import process_abstracts_data\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "import json\n",
    "from Bio import Entrez\n",
    "# import vllm\n",
    "# from lmformatenforcer import RegexParser\n",
    "# from lmformatenforcer.integrations.vllm import build_vllm_logits_processor, build_vllm_token_enforcer_tokenizer_data\n",
    "# import argparse\n",
    "from abstract_comprehension import read_tsv_to_dataframe\n",
    "from tqdm import tqdm\n",
    "\n",
    "def getHypothesis(config, b_term: str, a_term: str) -> str:\n",
    "    job_type = config.get(\"JOB_TYPE\", \"\").lower()\n",
    "    if job_type == \"km_with_gpt\":\n",
    "        hypothesis_template = config.get(\"KM_hypothesis\", \"\")\n",
    "    elif job_type == \"position_km_with_gpt\":\n",
    "        hypothesis_template = config.get(\"POSITION_KM_hypothesis\", \"\")\n",
    "    elif job_type == \"skim_with_gpt\":\n",
    "        hypothesis_template = config.get(\"SKIM_hypothesis\", \"\")\n",
    "    else:\n",
    "        return \"No valid hypothesis for the provided JOB_TYPE.\"\n",
    "    \n",
    "    return hypothesis_template.format(a_term=a_term, b_term=b_term)\n",
    "\n",
    "def cot_prompt(sys_prompt: str, hyp: str, abstract: str) -> str:\n",
    "  return f\"\"\"\n",
    "    <|im_start|>system\n",
    "    {sys_prompt}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Hypothesis: {hyp}\n",
    "    Abstract: {abstract}\n",
    "    \n",
    "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Let's work this out in a step by step way to be sure we have the right answer.\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "def answer_prompt(sys_prompt: str, hypothesis: str, abstract: str, chain_of_thought: str) -> str:\n",
    "    return f\"\"\"\n",
    "    <|im_start|>system\n",
    "    {sys_prompt}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Hypothesis: {hypothesis}\n",
    "    Abstract: {abstract}\n",
    "    \n",
    "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Let's work this out in a step by step way to be sure we have the right answer.\n",
    "    {chain_of_thought}\n",
    "    \n",
    "    Classify the given abstract as either 0 (Not relevant) or 1 (Relevant) based on your reasoning above and any information in the abstract and hypothesis.\n",
    "    Answer: \n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "# def gen(prompts: list[str], model: any, sampling_config: vllm.SamplingParams) -> list[str]:\n",
    "# \tgenerated = model.generate(prompts, sampling_params = sampling_config)\n",
    "# \toutputs = generated.output.outputs[0].text\n",
    "# \treturn outputs\n",
    "\n",
    "# Redefined reshape function to work with ragged string arrays\n",
    "def reshape(inp: list, shape: list) -> list:\n",
    "    assert(len(inp) == sum(shape))\n",
    "    output = []\n",
    "    running_length = 0;\n",
    "    for length in shape:\n",
    "        output.append(inp[running_length: running_length + length])\n",
    "        running_length = length\n",
    "        \n",
    "    return output\n",
    "\n",
    "def expand(inputs: list, shape_list: list) -> list:\n",
    "    assert(len(inputs) == len(shape_list))\n",
    "    expanded = []\n",
    "    for idx, inp in enumerate(inputs):\n",
    "        expanded.append([inp] * shape_list[idx])\n",
    "    return expanded\n",
    "        \n",
    "def flatten(inputs: list) -> list:\n",
    "    output = []\n",
    "    for lst in inputs:\n",
    "        output.extend(lst)\n",
    "    return output\n",
    "\n",
    "def getCoTPrompts(abstracts: list[str], sys_prompt: str, hypotheses: list[str]) -> list[str]:\n",
    "\treturn [cot_prompt(sys_prompt, hypotheses[i], abstract) for i, abstract in enumerate(abstracts)]\n",
    "\n",
    "def getAnswerPrompts(abstracts: list[str], sys_prompt: str, hypotheses: list[str], cot_outputs: list[str]) -> list[str]:\n",
    "\treturn [answer_prompt(sys_prompt, hypotheses[i], abstract, cot_outputs[i]) for i, abstract in enumerate(abstracts)]\n",
    "\n",
    "# Returns a dictionary for each PMID & Abstract Pair\n",
    "# This method is needed since Entrez automatically removes duplicates in the pmid list\n",
    "def getAbstractMap(config: json, pmids: list[str]) -> dict:\n",
    "    returned_pmids = []\n",
    "    returned_abstracts = []\n",
    "    global_config = config[\"GLOBAL_SETTINGS\"]\n",
    "    pmid_config = global_config[\"PUBMED_PARAMS\"]\n",
    "    \n",
    "    Entrez.api_key = pmid_config[\"api_key\"]\n",
    "    Entrez.max_tries = global_config[\"MAX_RETRIES\"]\n",
    "    Entrez.sleep_between_tries = global_config[\"RETRY_DELAY\"]\n",
    "    efetch = Entrez.efetch(db=pmid_config[\"db\"], id=pmids, rettype=pmid_config[\"rettype\"])\n",
    "    \n",
    "    output = Entrez.read(efetch)\n",
    "    efetch.close()\n",
    "    \n",
    "    min_word_count = config[\"GLOBAL_SETTINGS\"].get(\n",
    "        \"MIN_WORD_COUNT\", 100\n",
    "    )\n",
    "    \n",
    "    for paper in output[\"PubmedArticle\"]:\n",
    "        returned_pmids.append(str(paper[\"MedlineCitation\"][\"PMID\"]))\n",
    "        abstract_text = \" \".join(paper[\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"])\n",
    "        returned_abstracts.append(abstract_text)\n",
    "    return dict(zip(returned_pmids, returned_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "\tkm_output = \"../data.tsv\"\n",
    "\tconfig = \"../config.json\"\n",
    "\tfiltered_tsv_name = \"filtered.tsv\"\n",
    "\tcot_tsv_name = \"cot.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Data Loading & Processsing ############################ \n",
    "km_output = read_tsv_to_dataframe(args.km_output)\n",
    "with open(args.config) as f:\n",
    "\tconfig = json.load(f)\n",
    "filtered_tsv_name = args.filtered_tsv_name\n",
    "cot_tsv_name = args.cot_tsv_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_intersection = []\n",
    "shape = []\n",
    "for intersection in km_output.ab_pmid_intersection:\n",
    "\tab_intersection.extend(eval(intersection))\n",
    "\tshape.append(len(eval(intersection)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_map = process_abstracts_data(config, ab_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [abstract_map.get(str(pmid), \"\") for pmid in ab_intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_term = km_output.a_term.unique().tolist()[0].split(\"&\")[0]\n",
    "b_terms = km_output.b_term.unique().tolist()\n",
    "\n",
    "filter_config = config[\"abstract_filter\"]\n",
    "sys_prompt = filter_config['SYS_PROMPT']\n",
    "hypotheses = expand([getHypothesis(config, a_term, b_term) for b_term in b_terms], shape) # Done to make creating prompts easier\n",
    "hypotheses = flatten(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompts = getCoTPrompts(abstracts, sys_prompt, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompts = getAnswerPrompts(abstracts, sys_prompt, hypotheses, cot_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vllm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mistral \u001b[38;5;241m=\u001b[39m \u001b[43mvllm\u001b[49m\u001b[38;5;241m.\u001b[39mLLM(model\u001b[38;5;241m=\u001b[39mfilter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_model_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16832\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer_data \u001b[38;5;241m=\u001b[39m build_vllm_token_enforcer_tokenizer_data(mistral)\n\u001b[1;32m      3\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m build_vllm_logits_processor(tokenizer_data, RegexParser(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0|1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vllm' is not defined"
     ]
    }
   ],
   "source": [
    "mistral = vllm.LLM(model=filter_config[\"MODEL\"], max_model_len=16832)\n",
    "tokenizer_data = build_vllm_token_enforcer_tokenizer_data(mistral)\n",
    "logits_processor = build_vllm_logits_processor(tokenizer_data, RegexParser(r\"0|1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompts = getCoTPrompts(abstracts, sys_prompt, hypotheses)\n",
    "sampling_cot = vllm.SamplingParams(\n",
    "\t\t\ttemperature=filter_config[\"TEMPERATURE\"], \n",
    "\t\t\ttop_k = filter_config[\"TOP_K\"], top_p=filter_config[\"TOP_P\"], \n",
    "\t\t\trepetition_penalty=filter_config[\"REPETITION_PENALTY\"],\n",
    "\t\t\tmax_tokens = 1024)\n",
    "cot_outputs = gen(cot_prompts, mistral, sampling_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompts = getAnswerPrompts(abstracts, sys_prompt, hypotheses, cot_outputs)\n",
    "sampling_answer = vllm.SamplingParams(\n",
    "\t\t\ttemperature=filter_config[\"TEMPERATURE\"], \n",
    "\t\t\ttop_k = filter_config[\"TOP_K\"], top_p=filter_config[\"TOP_P\"], \n",
    "\t\t\tmax_tokens=1,\n",
    "\t\t\trepetition_penalty=filter_config[\"REPETITION_PENALTY\"],\n",
    "\t\t\tlogits_processors=[logits_processor])\n",
    "answers = gen(answer_prompts, mistral, sampling_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_tsv = km_output.copy(deep = True)\n",
    "filtered_tsv = km_output.copy(deep = True)\n",
    "\n",
    "answers = [eval(answer) for answer in answers] # Turn answers into list of ints\n",
    "shape = [len(abstract_list) for abstract_list in abstracts] # Get the shape of the abstracts\n",
    "answers = reshape(answers, shape)\n",
    "\n",
    "cot_tsv[\"scores\"] = answers\n",
    "cot_tsv[\"chain_of_thought\"] = reshape(cot_outputs, shape)\n",
    "cot_tsv[\"hypothesis\"] = hypotheses\n",
    "cot_tsv.to_csv(f\"{cot_tsv_name}\", sep='\\t')\n",
    "\n",
    "# Filter out the abstracts according to the scores\n",
    "filtered_abstracts = []\n",
    "for i, abstract_list in enumerate(abstracts):\n",
    "\tfiltered = []\n",
    "\tfor j, score in enumerate(answers[i]):\n",
    "\t\tif score == 1:\n",
    "\t\t\tfiltered.append(abstract_list[j])\n",
    "\tfiltered_abstracts.append(filtered)\n",
    "\n",
    "\n",
    "filtered_tsv[\"ab_pmid_intersection\"] = filtered_abstracts\n",
    "filtered_tsv.to_csv(f\"{filtered_tsv_name}\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
