{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "import json\n",
    "from Bio import Entrez\n",
    "# import vllm\n",
    "# from lmformatenforcer import RegexParser\n",
    "# from lmformatenforcer.integrations.vllm import build_vllm_logits_processor, build_vllm_token_enforcer_tokenizer_data\n",
    "import argparse\n",
    "from abstract_comprehension import read_tsv_to_dataframe\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils import Config, RaggedTensor\n",
    "\n",
    "# Returns either AB or BC hypotheses depending on the input. If A, B is passed in, getHypothesis will retrieve the AB hypothesis. \n",
    "# Only two arguements should be specified at once.\n",
    "def getHypothesis(config, a_term: str = None, b_term: str = None, c_term: str = None) -> str:\n",
    "    job_type = config.get(\"JOB_TYPE\", \"\").lower()\n",
    "    \n",
    "    if job_type == \"km_with_gpt\":\n",
    "        assert a_term and b_term and not c_term\n",
    "        hypothesis_template = config.get(\"KM_hypothesis\", \"\")\n",
    "        \n",
    "        return hypothesis_template.format(a_term=a_term, b_term=b_term)\n",
    "    \n",
    "    elif job_type == \"position_km_with_gpt\":\n",
    "        assert a_term and b_term and not c_term\n",
    "        \n",
    "        hypothesis_template = config.get(\"POSITION_KM_hypothesis\", \"\")\n",
    "        return hypothesis_template.format(a_term=a_term, b_term=b_term), None\n",
    "    \n",
    "    elif job_type == \"skim_with_gpt\":\n",
    "        assert (a_term and b_term and not c_term) or (b_term and c_term and not a_term)\n",
    "        \n",
    "        if a_term and b_term and not c_term:\n",
    "            hypothesis_template = config.get(\"SKIM_hypotheses\", \"\").get(\"AB\")\n",
    "            return hypothesis_template.format(a_term=a_term, b_term=b_term)\n",
    "        \n",
    "        elif b_term and c_term and not a_term:\n",
    "            hypothesis_template = config.get(\"SKIM_hypotheses\", \"\").get(\"BC\")\n",
    "            return hypothesis_template.format(b_term=b_term, c_term = c_term)\n",
    "\n",
    "    else:\n",
    "        return \"No valid hypothesis for the provided JOB_TYPE.\"\n",
    "    \n",
    "    \n",
    "\n",
    "def cot_prompt(sys_prompt: str, hyp: str, abstract: str) -> str:\n",
    "    return f\"\"\"\n",
    "    <|im_start|>system\n",
    "    {sys_prompt}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Hypothesis: {hyp}\n",
    "    Abstract: {abstract}\n",
    "\n",
    "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Make sure to stay focused on what the hypothesis is specifically saying. Let's work this out in a step by step way to be sure we have the right answer.\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "def answer_prompt(sys_prompt: str, hypothesis: str, abstract: str, chain_of_thought: str) -> str:\n",
    "    return f\"\"\"\n",
    "    <|im_start|>system\n",
    "    {sys_prompt}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Hypothesis: {hypothesis}\n",
    "    Abstract: {abstract}\n",
    "\n",
    "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
    "\n",
    "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Make sure to stay focused on what the hypothesis is specifically saying. Let's work this out in a step by step way to be sure we have the right answer.\n",
    "    {chain_of_thought}\n",
    "\n",
    "    Classify the given abstract as either 0 (Not relevant for scientifically assessing the hypothesis) or 1 (Relevant for scientifically assessing the hypothesis) based on the reasoning above and other useful pieces of information in the abstract and hypothesis. If an abstract is only somewhat relevant, consider it to be relevant.\n",
    "    Answer: \n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "# def gen(prompts: list[str], model: any, sampling_config: vllm.SamplingParams) -> list[str]:\n",
    "# \tgenerated = model.generate(prompts, sampling_params = sampling_config)\n",
    "# \toutputs = [output.outputs[0].text for output in generated]\n",
    "# \treturn outputs\n",
    "\n",
    "def getCoTPrompts(abstracts: RaggedTensor, sys_prompt: str, hypotheses: RaggedTensor) -> RaggedTensor:\n",
    "    assert not abstracts.is2D(), \"abstracts should be flattened.\"\n",
    "    assert not hypotheses.is2D(), \"hypotheses should be flattened.\"\n",
    "    return RaggedTensor([cot_prompt(sys_prompt, hypotheses[i], abstracts[i]) for i in range(abstracts.shape)], hypotheses.break_point)\n",
    "\n",
    "def getAnswerPrompts(abstracts: RaggedTensor, sys_prompt: str, hypotheses: RaggedTensor, cot_outputs: RaggedTensor) -> RaggedTensor:\n",
    "    assert not abstracts.is2D(), \"abstracts should be flattened.\"\n",
    "    assert not hypotheses.is2D(), \"hypotheses should be flattened.\"\n",
    "    assert not cot_outputs.is2D(), \"cot outputs should be flattened\"\n",
    "    return RaggedTensor([answer_prompt(sys_prompt, hypotheses[i], abstracts[i], cot_outputs[i]) for i in range(abstracts.shape)], hypotheses.break_point)\n",
    "    \n",
    "# Returns a dictionary for each PMID & Abstract Pair\n",
    "# This method is needed since Entrez automatically removes duplicates in the pmid list\n",
    "def getAbstractMap(config: json, pmids: list[str]) -> dict:\n",
    "    returned_pmids = []\n",
    "    returned_abstracts = []\n",
    "    global_config = config[\"GLOBAL_SETTINGS\"]\n",
    "    pmid_config = global_config[\"PUBMED_PARAMS\"]\n",
    "    \n",
    "    Entrez.email = 'leoxu27@gmail.com'\n",
    "    Entrez.api_key = pmid_config[\"api_key\"]\n",
    "    Entrez.max_tries = global_config[\"MAX_RETRIES\"]\n",
    "    Entrez.sleep_between_tries = global_config[\"RETRY_DELAY\"]\n",
    "    efetch = Entrez.efetch(db=pmid_config[\"db\"], id=pmids, rettype=pmid_config[\"rettype\"])\n",
    "    \n",
    "    output = Entrez.read(efetch)\n",
    "    efetch.close()\n",
    "    \n",
    "    for paper in output[\"PubmedArticle\"]:\n",
    "        returned_pmids.append(str(paper[\"MedlineCitation\"][\"PMID\"]))\n",
    "        abstract_text = \" \".join(paper[\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"])\n",
    "        returned_abstracts.append(abstract_text)\n",
    "    return dict(zip(returned_pmids, returned_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "\tkm_output = \"./abc.tsv\"\n",
    "\tconfig = \"../config.json\"\n",
    "\tfiltered_tsv_name = \"filtered.tsv\"\n",
    "\tcot_tsv_name = \"cot.tsv\"\n",
    " \n",
    "f = open(args.config)\n",
    "args.config = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job type detected. Running skim_with_gpt.\n"
     ]
    }
   ],
   "source": [
    "config = Config(args)\n",
    "cot_tsv = config.data.copy(deep = True)\n",
    "filtered_tsv = config.data.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_term = config.data.a_term.unique().tolist()[0].split(\"&\")[0]\n",
    "b_terms = config.data.b_term.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_pmids = RaggedTensor([eval(lst) for lst in config.data.ab_pmid_intersection])\n",
    "ab_hypotheses = RaggedTensor([getHypothesis(config.job_config, a_term = a_term, b_term = b_term) for b_term in b_terms])\n",
    "\n",
    "all_pmids = ab_pmids.flatten()\n",
    "all_hypotheses = ab_hypotheses.expand(ab_pmids.shape)\n",
    "\n",
    "if config.is_skim_gpt:\n",
    "\tc_term = config.data.c_term.unique().tolist()[0]\n",
    "\tbc_pmids = RaggedTensor([eval(lst) for lst in config.data.bc_pmid_intersection])\n",
    "\tbc_hypotheses = RaggedTensor([getHypothesis(config.job_config, c_term = c_term, b_term = b_term) for b_term in b_terms])\n",
    " \n",
    "\tall_pmids += bc_pmids.flatten()\n",
    "\tall_hypotheses += bc_hypotheses.expand(bc_pmids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_map = getAbstractMap(config.job_config, all_pmids)\n",
    "abstracts = all_pmids.map(lambda pmid: abstract_map.get(str(pmid), \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompts = getCoTPrompts(abstracts, config.sys_prompt, all_hypotheses)\n",
    "answer_prompts = getAnswerPrompts(abstracts, config.sys_prompt, all_hypotheses, cot_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = np.ones(40)\n",
    "answers[:20] = 0\n",
    "np.random.shuffle(answers)\n",
    "answers = RaggedTensor(list(answers), break_point = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_answers, bc_answers = answers.split()\n",
    "ab_abstracts, bc_abstracts = abstracts.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_answers.reshape(ab_pmids.shape)\n",
    "ab_abstracts.reshape(ab_pmids.shape)\n",
    "ab_abstracts.applyFilter(ab_answers)\n",
    "\n",
    "cot_tsv[\"ab_hypothesis\"] = ab_hypotheses.data\n",
    "cot_tsv[\"ab_scores\"] = ab_answers.data\n",
    "filtered_tsv[\"ab_pmid_intersection\"] = ab_abstracts.data\n",
    "\n",
    "if config.is_skim_gpt:\n",
    "\tbc_answers.reshape(bc_pmids.shape)\n",
    "\tbc_abstracts.reshape(bc_pmids.shape)\n",
    "\tbc_abstracts.applyFilter(bc_answers)\n",
    " \n",
    "\tfiltered_tsv[\"bc_pmid_intersection\"] = bc_abstracts.data\n",
    "\tcot_tsv[\"bc_hypothesis\"] = bc_hypotheses.data\n",
    "\tcot_tsv[\"bc_score\"] = bc_answers.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tsv.to_csv(f\"{config.filtered_tsv_name}\", sep=\"\\t\")\n",
    "cot_tsv.to_csv(f\"{config.cot_tsv_name}\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
