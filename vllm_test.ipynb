{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Getting Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "drAal8t77ssE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import set_seed\n",
        "import torch\n",
        "from typing import Callable\n",
        "import random\n",
        "import os\n",
        "from src.get_pubmed_text import process_abstracts_data\n",
        "import json\n",
        "import vllm\n",
        "from lmformatenforcer import RegexParser\n",
        "from lmformatenforcer.integrations.vllm import build_vllm_logits_processor, build_vllm_token_enforcer_tokenizer_data\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Configuration Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "3YHHLUIU7ssJ"
      },
      "outputs": [],
      "source": [
        "# Configuration variables for this whole notebook\n",
        "class config:\n",
        "    model = \"Mistral-7B-OpenOrca\"\n",
        "    top_k = 20\n",
        "    top_p = 0.95\n",
        "    max_new_tokens = 500\n",
        "    temperature = 0.8\n",
        "    repetition_penalty = 1.2\n",
        "    frequency_penalty = 1.2\n",
        "    max_tokens = 2048\n",
        "    batch_size = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9MIqG1I8zbs",
        "outputId": "e663b783-2bf5-426c-889c-bd2c4b646514"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get install git-lfs\n",
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "hY3m7O4H7ssN",
        "outputId": "ce9f73d0-2d7c-4d28-8e82-8374f0801e6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_count</th>\n",
              "      <th>a_term</th>\n",
              "      <th>ab_count</th>\n",
              "      <th>ab_pmid_intersection</th>\n",
              "      <th>ab_pred_score</th>\n",
              "      <th>ab_pvalue</th>\n",
              "      <th>ab_sort_ratio</th>\n",
              "      <th>b_count</th>\n",
              "      <th>b_term</th>\n",
              "      <th>total_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>52</td>\n",
              "      <td>[15911722, 7944078, 24550106, 20002088, 256468...</td>\n",
              "      <td>0.330813</td>\n",
              "      <td>2.689688e-101</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>9013</td>\n",
              "      <td>omeprazole</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>42</td>\n",
              "      <td>[10709776, 12036392, 15871634, 15568889, 15260...</td>\n",
              "      <td>0.243054</td>\n",
              "      <td>5.577908e-75</td>\n",
              "      <td>0.003910</td>\n",
              "      <td>10743</td>\n",
              "      <td>simvastatin</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>26</td>\n",
              "      <td>[7944078, 10709776, 9512916, 8801057, 18685566...</td>\n",
              "      <td>0.121822</td>\n",
              "      <td>3.842457e-38</td>\n",
              "      <td>0.001779</td>\n",
              "      <td>14616</td>\n",
              "      <td>fluconazole</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>16</td>\n",
              "      <td>[25646891, 7429002, 3395358, 9667024, 20489028...</td>\n",
              "      <td>0.068799</td>\n",
              "      <td>1.350138e-21</td>\n",
              "      <td>0.001245</td>\n",
              "      <td>12852</td>\n",
              "      <td>furosemide</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>8</td>\n",
              "      <td>[10709776, 20002088, 6096071, 8793611, 8793602...</td>\n",
              "      <td>0.034793</td>\n",
              "      <td>7.571592e-11</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>7778</td>\n",
              "      <td>metoprolol</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>4</td>\n",
              "      <td>[22250655, 22406649, 32862668, 34691471]</td>\n",
              "      <td>0.017386</td>\n",
              "      <td>1.583282e-05</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>5329</td>\n",
              "      <td>enoxaparin</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>3</td>\n",
              "      <td>[22794158, 32982467, 34691471]</td>\n",
              "      <td>0.007539</td>\n",
              "      <td>6.358695e-03</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>13669</td>\n",
              "      <td>ceftriaxone</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>989</td>\n",
              "      <td>warfarin&amp;drug&amp;interaction</td>\n",
              "      <td>16</td>\n",
              "      <td>[8792056, 16697485, 21053990, 21253716, 112482...</td>\n",
              "      <td>0.027246</td>\n",
              "      <td>2.550025e-09</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>83972</td>\n",
              "      <td>heparin</td>\n",
              "      <td>36618932</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_count                     a_term  ab_count  \\\n",
              "0      989  warfarin&drug&interaction        52   \n",
              "1      989  warfarin&drug&interaction        42   \n",
              "2      989  warfarin&drug&interaction        26   \n",
              "3      989  warfarin&drug&interaction        16   \n",
              "4      989  warfarin&drug&interaction         8   \n",
              "5      989  warfarin&drug&interaction         4   \n",
              "6      989  warfarin&drug&interaction         3   \n",
              "7      989  warfarin&drug&interaction        16   \n",
              "\n",
              "                                ab_pmid_intersection  ab_pred_score  \\\n",
              "0  [15911722, 7944078, 24550106, 20002088, 256468...       0.330813   \n",
              "1  [10709776, 12036392, 15871634, 15568889, 15260...       0.243054   \n",
              "2  [7944078, 10709776, 9512916, 8801057, 18685566...       0.121822   \n",
              "3  [25646891, 7429002, 3395358, 9667024, 20489028...       0.068799   \n",
              "4  [10709776, 20002088, 6096071, 8793611, 8793602...       0.034793   \n",
              "5           [22250655, 22406649, 32862668, 34691471]       0.017386   \n",
              "6                     [22794158, 32982467, 34691471]       0.007539   \n",
              "7  [8792056, 16697485, 21053990, 21253716, 112482...       0.027246   \n",
              "\n",
              "       ab_pvalue  ab_sort_ratio  b_count       b_term  total_count  \n",
              "0  2.689688e-101       0.005769     9013   omeprazole     36618932  \n",
              "1   5.577908e-75       0.003910    10743  simvastatin     36618932  \n",
              "2   3.842457e-38       0.001779    14616  fluconazole     36618932  \n",
              "3   1.350138e-21       0.001245    12852   furosemide     36618932  \n",
              "4   7.571592e-11       0.001029     7778   metoprolol     36618932  \n",
              "5   1.583282e-05       0.000751     5329   enoxaparin     36618932  \n",
              "6   6.358695e-03       0.000219    13669  ceftriaxone     36618932  \n",
              "7   2.550025e-09       0.000191    83972      heparin     36618932  "
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "km_output = pd.read_csv(\"./data.tsv\", sep = \"\\t\")\n",
        "km_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getHypothesis(a_term: str, b_term: str) -> str:\n",
        "    return f\"{b_term} will have a drug-drug interaction with {a_term}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "GfOX0d3G7ssO"
      },
      "outputs": [],
      "source": [
        "def cot_prompt(sys_prompt: str, hyp: str, abstract: str) -> str:\n",
        "  return f\"\"\"\n",
        "    <|im_start|>system\n",
        "    {sys_prompt}\n",
        "    <|im_end|>\n",
        "    <|im_start|>user\n",
        "    Hypothesis: {{ hypothesis }}\n",
        "    Abstract: {{abstract}}\n",
        "    \n",
        "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
        "\n",
        "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Let's work this out in a step by step way to be sure we have the right answer.\n",
        "\t<|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_prompt(sys_prompt: str, hypothesis: str, abstract: str, chain_of_thought: str) -> str:\n",
        "    return f\"\"\"\n",
        "    <|im_start|>system\n",
        "    {sys_prompt}\n",
        "    <|im_end|>\n",
        "    <|im_start|>user\n",
        "    Hypothesis: {hypothesis}\n",
        "    Abstract: {abstract}\n",
        "    \n",
        "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
        "\n",
        "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Let's work this out in a step by step way to be sure we have the right answer.\n",
        "    {chain_of_thought}\n",
        "    \n",
        "    Classify the given abstract as either 0 (Not relevant) or 1 (Relevant) based on your reasoning above and any information in the abstract and hypothesis.\n",
        "    Answer: \n",
        "    <|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For a batched input generate output with flattened dimensions\n",
        "def gen(batches: list[str], model: any, sampling_config: vllm.SamplingParams) -> list[str]:\n",
        "    outputs = []\n",
        "    for batch in batches:\n",
        "        generated = model.generate(batch, sampling_params = sampling_config)\n",
        "        outputs.extend([output.outputs[0].text for output in generated])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(inp: list, batch_size: int) -> list:\n",
        "    return [inp[i * batch_size:(i + 1) * batch_size] for i in range((len(inp) + batch_size - 1) // batch_size )]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Redefined reshape function to work with ragged string arrays\n",
        "def reshape(inp: list, shape: list) -> list:\n",
        "    assert(len(inp) == sum(shape))\n",
        "    output = []\n",
        "    running_length = 0;\n",
        "    for length in shape:\n",
        "        output.append(inp[running_length: running_length + length])\n",
        "        running_length = length\n",
        "        \n",
        "    return output\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "w_GW5IJM-4fp"
      },
      "outputs": [],
      "source": [
        "# Total of 65 abstracts here \n",
        "with open(\"/home/ubuntu/kmGPT/config.json\") as file:\n",
        "\tjob_config = json.load(file)\n",
        " \n",
        "b_terms_pmids = km_output.ab_pmid_intersection.map(lambda pmid_list: pmid_list.strip('][').split(', '))\n",
        "# Grab only the abstract from each list of pmids in the TSV\n",
        "abstracts = [process_abstracts_data(job_config, pmid_list)[0] for pmid_list in b_terms_pmids] # Fetch abstracts from each b_term's PMID list\n",
        "\n",
        "# There should only be one a_term, so it's safe to grab the first index\n",
        "a_term = km_output.a_term.unique().tolist()[0].split(\"&\")[0]\n",
        "b_terms = km_output.b_term.unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys_prompt = \"You are an incredibly brilliant biomedical researcher who has spent their lifetime reading all the papers in PubMed. You are focused on uplifting other researchers in dire need to evaluate suggested hypotheses given abstracts in PubMed. The sole purpose of your existence is to help uncover hidden connections between the work of existing papers, examining the fully connected relationship between papers while maintaining a strict standard of truth.\"\n",
        "hypotheses = [getHypothesis(a_term, b_term) for b_term in b_terms]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0x1A4_l8uvm"
      },
      "source": [
        "### 2) Model Inference\n",
        "#### Techniques Used\n",
        "1. Zero-Shot CoT Prompting\n",
        "2. Constrained generation\n",
        "3. Prompt chaining\n",
        "4. GPU Batching\n",
        "5. Paged Attention!\n",
        "6. Special sampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1) Chain of Thought Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 03-06 17:27:26 llm_engine.py:87] Initializing an LLM engine with config: model='Mistral-7B-OpenOrca', tokenizer='Mistral-7B-OpenOrca', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16832, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 03-06 17:27:40 llm_engine.py:357] # GPU blocks: 2054, # CPU blocks: 2048\n",
            "INFO 03-06 17:27:41 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 03-06 17:27:41 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 03-06 17:27:45 model_runner.py:756] Graph capturing finished in 4 secs.\n"
          ]
        }
      ],
      "source": [
        "mistral = vllm.LLM(model=\"Mistral-7B-OpenOrca\", max_model_len=16832)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "cot_prompts = [cot_prompt(sys_prompt, hypotheses[i], abstract) for i, abstract_list in enumerate(abstracts) for abstract in abstract_list]\n",
        "cot_batches = get_batch(cot_prompts, config.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "lpx6u4j27ssM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 32/32 [00:20<00:00,  1.53it/s]\n",
            "Processed prompts: 100%|██████████| 32/32 [00:23<00:00,  1.38it/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.14s/it]\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "sampling_cot = vllm.SamplingParams(\n",
        "\t\t\ttemperature=config.temperature, \n",
        "\t\t\ttop_k = config.top_k, top_p=config.top_p, \n",
        "\t\t\tmax_tokens=config.max_tokens, \n",
        "\t\t\trepetition_penalty=config.repetition_penalty)\n",
        "cot_outputs = gen(cot_batches, mistral, sampling_cot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2) Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_data = build_vllm_token_enforcer_tokenizer_data(mistral)\n",
        "logits_processor = build_vllm_logits_processor(tokenizer_data, RegexParser(r\"0|1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer_prompts = []\n",
        "total_idx = 0\n",
        "for i, abstract_list in enumerate(abstracts):\n",
        "    for j, abstract in enumerate(abstract_list):\n",
        "        answer_prompts.append(answer_prompt(sys_prompt, hypotheses[i], abstract, cot_outputs[total_idx + j]))\n",
        "    total_idx += len(abstract_list)\n",
        "answer_batches = get_batch(answer_prompts, config.batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.46it/s]\n",
            "Processed prompts: 100%|██████████| 32/32 [00:08<00:00,  3.88it/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n"
          ]
        }
      ],
      "source": [
        "sampling_answer = vllm.SamplingParams(\n",
        "\t\t\ttemperature=config.temperature, \n",
        "\t\t\ttop_k = config.top_k, top_p=config.top_p, \n",
        "\t\t\tmax_tokens=config.max_tokens, \n",
        "\t\t\trepetition_penalty=config.repetition_penalty,\n",
        "   \t\t\tlogits_processors=[logits_processor])\n",
        "answers = gen(answer_batches, mistral, sampling_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers = [eval(answer) for answer in answers]\n",
        "shape = [len(abstract_list) for abstract_list in abstracts]\n",
        "answers = reshape(answers, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
              " [0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
              " [0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
              " [0, 0, 1, 0, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0],\n",
              " [0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0, 1, 0, 0, 1]]"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Post Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1) Generate Chain of Thought TSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "OJXlWKf57ssP"
      },
      "outputs": [],
      "source": [
        "cot_tsv = km_output.copy(deep = True)\n",
        "cot_tsv[\"scores\"] = answers\n",
        "cot_tsv[\"chain_of_thought\"] = reshape(cot_outputs, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [],
      "source": [
        "cot_tsv.to_csv(\"chain_of_thought.tsv\", sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2) Generate Output TSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_tsv = km_output.copy(deep = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_abstracts = []\n",
        "for i, abstract_list in enumerate(abstracts):\n",
        "\tfor j, score in enumerate(answers[i]):\n",
        "\t\tfiltered = []\n",
        "\t\tif score == 1:\n",
        "\t\t\tfiltered.append(abstract_list[j])\n",
        "\tfiltered_abstracts.append(filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_tsv[\"ab_pmid_intersection\"] = filtered_abstracts\n",
        "filtered_tsv.to_csv(\"filtered_output.tsv\", sep=\"\\t\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
