{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OPvRG7pM76Of",
        "outputId": "ec766f4c-6630-446f-b22a-95ca27f3b5cd"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "drAal8t77ssE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import set_seed\n",
        "import torch\n",
        "from typing import Callable\n",
        "import random\n",
        "import os\n",
        "from src.get_pubmed_text import process_abstracts_data\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3YHHLUIU7ssJ"
      },
      "outputs": [],
      "source": [
        "# Configuration variables for this whole notebook\n",
        "class config:\n",
        "    seed = 42\n",
        "    model = \"Mistral-7B-OpenOrca\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "    top_k = 20\n",
        "    top_p = 0.95\n",
        "    do_sample = True\n",
        "    num_return_sequences = 1\n",
        "    max_new_tokens = 500\n",
        "    temperature = 0.8\n",
        "    repetition_penalty = 1.2\n",
        "    penalty_alpha=0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9MIqG1I8zbs",
        "outputId": "e663b783-2bf5-426c-889c-bd2c4b646514"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get install git-lfs\n",
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hY3m7O4H7ssN",
        "outputId": "ce9f73d0-2d7c-4d28-8e82-8374f0801e6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>pmid</th>\n",
              "      <th>abstract</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>23956253</td>\n",
              "      <td>PMID: 23956253 Text: The first aim was to crit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>23444397</td>\n",
              "      <td>PMID: 23444397 Text: Niacin has potentially fa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>28886926</td>\n",
              "      <td>PMID: 28886926 Text: In 2016, the American Col...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>27701660</td>\n",
              "      <td>PMID: 27701660 Text: Low-density lipoprotein c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>19095139</td>\n",
              "      <td>PMID: 19095139 Text: This secondary analysis f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>21095263</td>\n",
              "      <td>PMID: 21095263 Text: Lowering low-density lipo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0      pmid                                           abstract  \\\n",
              "0           0  23956253  PMID: 23956253 Text: The first aim was to crit...   \n",
              "1           1  23444397  PMID: 23444397 Text: Niacin has potentially fa...   \n",
              "2           2  28886926  PMID: 28886926 Text: In 2016, the American Col...   \n",
              "3           3  27701660  PMID: 27701660 Text: Low-density lipoprotein c...   \n",
              "4           4  19095139  PMID: 19095139 Text: This secondary analysis f...   \n",
              "5           5  21095263  PMID: 21095263 Text: Lowering low-density lipo...   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      1  \n",
              "4      0  \n",
              "5      0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# km_output = pd.read_csv(\"./km_output.tsv\", sep = \"\\t\")\n",
        "# km_output\n",
        "km_output = pd.read_csv(\"./data.csv\")\n",
        "km_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"/home/ubuntu/kmGPT/config.json\") as file:\n",
        "\tjob_config = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w_GW5IJM-4fp"
      },
      "outputs": [],
      "source": [
        "# Total of 65 abstracts here \n",
        "\n",
        "# b_terms_pmids = km_output.ab_pmid_intersection.map(lambda pmid_list: pmid_list.strip('][').split(', '))\n",
        "# # Grab only the abstract from each list of pmids in the TSV\n",
        "# abstracts = [process_abstracts_data(job_config, pmid_list)[0] for pmid_list in b_terms_pmids] # Fetch abstracts from each b_term's PMID list\n",
        "# # There should only be one a_term, so it's safe to grab the first index\n",
        "# a_term = km_output.a_term.unique().tolist()[0].split(\"&\")[0]\n",
        "# b_terms = km_output.b_term.unique().tolist()\n",
        "# b_terms_pmids = km_output.ab_pmid_intersection.map(lambda pmid_list: pmid_list.strip('][').split(', '))\n",
        "# # Grab only the abstract from each list of pmids in the TSV\n",
        "# abstracts = [process_abstracts_data(job_config, pmid_list)[0] for pmid_list in b_terms_pmids] # Fetch abstracts from each b_term's PMID list\n",
        "# # There should only be one a_term, so it's safe to grab the first index\n",
        "# a_term = km_output.a_term.unique().tolist()[0].split(\"&\")[0]\n",
        "# b_terms = km_output.b_term.unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "_IuK5V9m7ssO"
      },
      "outputs": [],
      "source": [
        "hyp = \"ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes potentially offering therapeutic benefits or slowing disease progression.\"\n",
        "sys_prompt = \"You are an incredibly brilliant biomedical researcher who has spent their lifetime reading all the papers in PubMed. You are focused on uplifting other researchers in dire need to evaluate suggested hypotheses given abstracts in PubMed. The sole purpose of your existence is to help uncover hidden connections between the work of existing papers, examining the fully connected relationship between papers while maintaining a strict standard of truth.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "GfOX0d3G7ssO"
      },
      "outputs": [],
      "source": [
        "def retrieveZeroShotCoTPrompt(hyp: str, abstract: str) -> str:\n",
        "  zero_shot_prompt = f\"\"\"\n",
        "    Hypothesis: {hyp}\n",
        "    Abstract: {abstract}\n",
        "\n",
        "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis.\n",
        "    A relevant abstract should either support the given hypothesis or have evidence to refute the hypothesis.\n",
        "    A relevant abstract must directly comment on the hypothesis.\n",
        "\n",
        "    Let us think through this step by step.\n",
        "  \"\"\"\n",
        "  return zero_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = km_output.copy(deep = True)\n",
        "chain_of_thought = km_output.copy(deep=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>pmid</th>\n",
              "      <th>abstract</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>23956253</td>\n",
              "      <td>PMID: 23956253 Text: The first aim was to crit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>23444397</td>\n",
              "      <td>PMID: 23444397 Text: Niacin has potentially fa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>28886926</td>\n",
              "      <td>PMID: 28886926 Text: In 2016, the American Col...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>27701660</td>\n",
              "      <td>PMID: 27701660 Text: Low-density lipoprotein c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>19095139</td>\n",
              "      <td>PMID: 19095139 Text: This secondary analysis f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>21095263</td>\n",
              "      <td>PMID: 21095263 Text: Lowering low-density lipo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0      pmid                                           abstract  \\\n",
              "0           0  23956253  PMID: 23956253 Text: The first aim was to crit...   \n",
              "1           1  23444397  PMID: 23444397 Text: Niacin has potentially fa...   \n",
              "2           2  28886926  PMID: 28886926 Text: In 2016, the American Col...   \n",
              "3           3  27701660  PMID: 27701660 Text: Low-density lipoprotein c...   \n",
              "4           4  19095139  PMID: 19095139 Text: This secondary analysis f...   \n",
              "5           5  21095263  PMID: 21095263 Text: Lowering low-density lipo...   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      1  \n",
              "4      0  \n",
              "5      0  "
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0x1A4_l8uvm"
      },
      "source": [
        "# Outlines Test\n",
        "### Techniques Used\n",
        "1. Zero-Shot CoT Prompting\n",
        "2. llama.cpp GPU inference\n",
        "3. Constrained generation\n",
        "4. Prompt chaining\n",
        "### Unfortunately no batching since batching is pretty difficult with constrained generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "import outlines\n",
        "from llama_cpp import Llama\n",
        "from pydantic import BaseModel, field_validator, Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "@outlines.prompt\n",
        "def cot_prompt(sys_prompt, hypothesis, abstract):\n",
        "    \"\"\"\n",
        "    <|im_start|>system\n",
        "    {{ sys_prompt }}\n",
        "    <|im_end|>\n",
        "    <|im_start|>user\n",
        "    Hypothesis: {{ hypothesis }}\n",
        "    Abstract: {{abstract}}\n",
        "    \n",
        "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
        "\n",
        "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Let's work this out in a step by step way to be sure we have the right answer.\n",
        "\t<|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "@outlines.prompt\n",
        "def answer_prompt(sys_prompt, hypothesis, abstract, chain_of_thought):\n",
        "    \"\"\"\n",
        "    <|im_start|>system\n",
        "    {{ sys_prompt }}\n",
        "    <|im_end|>\n",
        "    <|im_start|>user\n",
        "    Hypothesis: {{ hypothesis }}\n",
        "    Abstract: {{abstract}}\n",
        "    \n",
        "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis. A relevant abstract must directly comment on the hypothesis and either support the given hypothesis or have evidence to refute the hypothesis.\n",
        "\n",
        "    Analyze the abstract above, and throughly describe your thought process for evaluating the hypothesis. Pay attention to particular details in the abstract as it relates to the hypothesis. Let's work this out in a step by step way to be sure we have the right answer.\n",
        "    {{chain_of_thought}}\n",
        "    \n",
        "    Classify the given abstract as either 0 (Not relevant) or 1 (Relevant) based on your reasoning above and any information in the abstract and hypothesis.\n",
        "    Answer: \n",
        "    <|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/ubuntu/kmGPT/mistral-7b-openorca.Q5_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = open-orca_mistral-7b-openorca\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
            "llm_load_print_meta: general.name     = open-orca_mistral-7b-openorca\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<dummy32000>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    85.94 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4807.06 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    17.04 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'open-orca_mistral-7b-openorca', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "model = outlines.models.llamacpp(model_name=\"/home/ubuntu/kmGPT/mistral-7b-openorca.Q5_K_M.gguf\", device = \"cuda\", model_kwargs = {\"n_gpu_layers\":-1, \"n_ctx\":4096, \"n_threads\": 30})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompts = [[zero_shot_cot_prompt(sys_prompt, hyp, abstract) for abstract in abstract_list] for abstract_list in abstracts]\n",
        "cot_prompts = [cot_prompt(sys_prompt, hyp, abstract) for abstract in km_output[\"abstract\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "lpx6u4j27ssM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.01 µs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =      81.25 ms /   349 runs   (    0.23 ms per token,  4295.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     352.63 ms /   790 tokens (    0.45 ms per token,  2240.28 tokens per second)\n",
            "llama_print_timings:        eval time =    5198.25 ms /   348 runs   (   14.94 ms per token,    66.95 tokens per second)\n",
            "llama_print_timings:       total time =    6252.79 ms /  1138 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =      87.05 ms /   374 runs   (    0.23 ms per token,  4296.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     409.53 ms /   920 tokens (    0.45 ms per token,  2246.51 tokens per second)\n",
            "llama_print_timings:        eval time =    5640.85 ms /   373 runs   (   15.12 ms per token,    66.12 tokens per second)\n",
            "llama_print_timings:       total time =    6809.70 ms /  1293 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =      94.39 ms /   407 runs   (    0.23 ms per token,  4311.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     423.01 ms /   969 tokens (    0.44 ms per token,  2290.71 tokens per second)\n",
            "llama_print_timings:        eval time =    6148.72 ms /   406 runs   (   15.14 ms per token,    66.03 tokens per second)\n",
            "llama_print_timings:       total time =    7401.77 ms /  1375 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =     121.64 ms /   525 runs   (    0.23 ms per token,  4315.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     526.53 ms /  1099 tokens (    0.48 ms per token,  2087.26 tokens per second)\n",
            "llama_print_timings:        eval time =    7969.39 ms /   524 runs   (   15.21 ms per token,    65.75 tokens per second)\n",
            "llama_print_timings:       total time =    9611.33 ms /  1623 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =     149.85 ms /   647 runs   (    0.23 ms per token,  4317.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     381.15 ms /   870 tokens (    0.44 ms per token,  2282.56 tokens per second)\n",
            "llama_print_timings:        eval time =    9802.41 ms /   646 runs   (   15.17 ms per token,    65.90 tokens per second)\n",
            "llama_print_timings:       total time =   11584.53 ms /  1516 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =     157.67 ms /   679 runs   (    0.23 ms per token,  4306.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     374.28 ms /   832 tokens (    0.45 ms per token,  2222.93 tokens per second)\n",
            "llama_print_timings:        eval time =   10311.82 ms /   678 runs   (   15.21 ms per token,    65.75 tokens per second)\n",
            "llama_print_timings:       total time =   12166.74 ms /  1510 tokens\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "cot_generator = outlines.generate.text(model, sampler=outlines.samplers.MultinomialSampler(temperature=0.8, top_k=20, top_p=0.95))\n",
        "cot_outputs = cot_generator(cot_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.29 µs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4535.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     554.06 ms /  1180 tokens (    0.47 ms per token,  2129.72 tokens per second)\n",
            "llama_print_timings:        eval time =      16.60 ms /     1 runs   (   16.60 ms per token,    60.23 tokens per second)\n",
            "llama_print_timings:       total time =     577.46 ms /  1181 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.22 ms per token,  4454.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     621.12 ms /  1335 tokens (    0.47 ms per token,  2149.33 tokens per second)\n",
            "llama_print_timings:        eval time =      15.84 ms /     1 runs   (   15.84 ms per token,    63.14 tokens per second)\n",
            "llama_print_timings:       total time =     643.88 ms /  1336 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4555.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     653.15 ms /  1417 tokens (    0.46 ms per token,  2169.50 tokens per second)\n",
            "llama_print_timings:        eval time =      16.12 ms /     1 runs   (   16.12 ms per token,    62.02 tokens per second)\n",
            "llama_print_timings:       total time =     676.92 ms /  1418 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.22 ms per token,  4454.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     800.34 ms /  1665 tokens (    0.48 ms per token,  2080.36 tokens per second)\n",
            "llama_print_timings:        eval time =      16.64 ms /     1 runs   (   16.64 ms per token,    60.11 tokens per second)\n",
            "llama_print_timings:       total time =     826.48 ms /  1666 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.23 ms per token,  4424.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     799.59 ms /  1558 tokens (    0.51 ms per token,  1948.51 tokens per second)\n",
            "llama_print_timings:        eval time =      16.03 ms /     1 runs   (   16.03 ms per token,    62.37 tokens per second)\n",
            "llama_print_timings:       total time =     823.92 ms /  1559 tokens\n",
            "\n",
            "llama_print_timings:        load time =     197.89 ms\n",
            "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.22 ms per token,  4454.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     770.29 ms /  1552 tokens (    0.50 ms per token,  2014.82 tokens per second)\n",
            "llama_print_timings:        eval time =      15.75 ms /     1 runs   (   15.75 ms per token,    63.49 tokens per second)\n",
            "llama_print_timings:       total time =     794.24 ms /  1553 tokens\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "answer_prompts = [answer_prompt(sys_prompt, hyp, km_output[\"abstract\"][i], cot_outputs[i]) for i in range(len(km_output[\"abstract\"]))]\n",
        "answer_generator = outlines.generate.choice(model, [\"0\", \"1\"])\n",
        "answers = answer_generator(answer_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0', '0', '0', '1', '1', '0']"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"\\n To determine if this abstract is relevant for evaluating the hypothesis, let's first understand the hypothesis and break it down into its components:\\n\\nHypothesis: Ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes potentially offering therapeutic benefits or slowing disease progression.\\n\\nNow, let's analyze the abstract for any direct mention or evidence that supports or refutes this hypothesis:\\n\\nAbstract Summary: The primary focus of the abstract revolves around the diagnosis, screening, and treatment of familial hypercholesterolemia (FH) and its associated risks of coronary heart disease (CHD). It provides specific recommendations for screening, cholesterol targets, and treatment options, including ezetimibe, for both children and adults with FH.\\n\\nOur thought process:\\n1. We look for any mention or discussion of diabetes in the abstract.\\n2. We search for evidence related to ezetimibe's potential role in alleviating pathogenic mechanisms of diabetes, offering therapeutic benefits, or slowing disease progression.\\n\\nUpon analyzing the abstract, we find that it focuses primarily on FH and CHD, but it does mention ezetimibe as a treatment option for adults with FH, along with statins and bile acid binding resins. However, there is no direct mention or evidence related to ezetimibe's potential role in alleviating pathogenic mechanisms of diabetes or offering therapeutic benefits for diabetes specifically. Therefore, based on our analysis, the provided abstract is not directly relevant for evaluating the given hypothesis.\",\n",
              " \"\\n To determine if the given abstract is relevant for evaluating the provided hypothesis, let's break down the hypothesis and analyze the content of the abstract.\\n\\nHypothesis: Ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes, potentially offering therapeutic benefits or slowing disease progression.\\n\\nThe hypothesis states that ezetimibe may have a positive impact on diabetes by potentially slowing disease progression or offering therapeutic benefits.\\n\\nNow, let's examine the information provided in the abstract:\\n1. The patients in this study were given simvastatin 40 mg plus, if required, ezetimibe 10 mg daily to standardize their low-density lipoprotein (LDL)-lowering therapy before randomization.\\n2. Ezetimibe was added to statin-based LDL-lowering therapy.\\n3. The risk of myopathy was increased by adding ERN/LRPT (which contains ezetimibe) to simvastatin 40 mg daily, particularly in Chinese patients whose myopathy rates on simvastatin were higher.\\n4. Despite side effects, among individuals who were able to tolerate ERN/LRPT, three-quarters continued taking it for about 4 years.\\n\\nThe abstract does provide some information about ezetimibe (added to statin-based LDL-lowering therapy) and its impact on patients with regard to myopathy, but it does not directly discuss its effects or potential benefits on diabetes, nor does it provide any evidence supporting or refuting the hypothesis. Therefore, based on the information provided in this abstract, it cannot be concluded that it is relevant for evaluating the provided hypothesis.\",\n",
              " \"\\n To determine if this abstract is relevant for evaluating the hypothesis, let's first understand the hypothesis and break it down into its components:\\n\\nHypothesis: Ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes potentially offering therapeutic benefits or slowing disease progression.\\n\\nNow, let's analyze the abstract for any connections to this hypothesis:\\n\\nAbstract: The main topic of this abstract revolves around providing updates for expert consensus decision pathway (ECDP) on the role of non-statin therapies for low-density lipoprotein (LDL)-cholesterol lowering in the management of atherosclerotic cardiovascular disease (ASCVD) risk. It discusses the role of ezetimibe and PCSK9 inhibitors and provides recommendations for their use in patients with clinical ASCVD with or without comorbidities. It also provides new information about diagnostic categories of heterozygous and homozygous familial hypercholesterolemia, based on clinical criteria with and without genetic testing.\\n\\nFrom the analysis, we can see that the hypothesis focuses on ezetimibe's effectiveness against diabetes pathogenic mechanisms, while the abstract primarily discusses non-statin therapies for atherosclerotic cardiovascular disease risk, ezetimibe, and PCSK9 inhibitors' roles and recommendations for their usage. Although there may be some connection between ezetimibe and diabetes, the main focus of the abstract is not on diabetes or its pathogenic mechanisms.\\n\\nTherefore, based on our analysis, this abstract is not highly relevant for evaluating the given hypothesis, as it mainly focuses on recommendations for non-statin therapies and atherosclerotic cardiovascular disease risk rather than directly discussing ezetimibe's effectiveness against diabetes pathogenic mechanisms.\",\n",
              " \"\\n To evaluate whether ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes, we will focus on the abstract's content and its relevance to the hypothesis. The hypothesis suggests that ezetimibe may be effective for alleviating diabetes-related pathogenic mechanisms and offering therapeutic benefits.\\n\\nStep 1: Identifying key phrases in the hypothesis:\\n- ezetimibe\\n- alleviate key pathogenic mechanisms of diabetes\\n- therapeutic benefits\\n- slowing disease progression\\n\\nStep 2: Identifying key phrases in the abstract:\\n- LDL-C-lowering alleles\\n- NPC1L1, HMGCR, PCSK9, ABCG5/G8, LDLR (genes related to ezetimibe and statins)\\n- type 2 diabetes\\n- coronary artery disease (CAD)\\n- associations with type 2 diabetes and CAD of LDL-C-lowering genetic variants\\n- NPC1L1 genetic variants were inversely associated with coronary artery disease (CAD) and directly associated with type 2 diabetes\\n- genetic associations with metabolic risk of LDL-C-lowering alleles\\n\\nStep 3: Comparing key phrases and relevancy:\\n- Ezetimibe is related to NPC1L1, which is one of the genes mentioned in the abstract.\\n- The association between LDL-C-lowering genetic variants and type 2 diabetes is mentioned in the abstract, supporting the hypothesis that ezetimibe may alleviate key pathogenic mechanisms of diabetes.\\n- However, the abstract also states that NPC1L1 genetic variants were inversely associated with CAD, which implies that a decrease in LDL-C does not always result in a beneficial effect, suggesting a potential risk associated with LDL-C-lowering therapy.\\n- The hypothesis suggests that ezetimibe may offer therapeutic benefits or slow down disease progression, but the abstract does not provide direct evidence supporting this claim.\\n\\nConclusion: Although there is some support for ezetimibe's association with type 2 diabetes, the available evidence from the provided abstract does not offer strong support for the hypothesis that ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes, potentially offering therapeutic benefits or slowing disease progression.\",\n",
              " '\\n To determine if this abstract is relevant for evaluating the provided hypothesis, let\\'s first break down the hypothesis and then analyze the abstract for any relevant information.\\n\\nHypothesis: Ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes, potentially offering therapeutic benefits or slowing disease progression.\\n\\nNow let\\'s analyze the abstract:\\n\\n1. The abstract describes the SANDS (Stop Atherosclerosis in Native Diabetics Study) trial, which aimed to compare the effects of lowering LDL-C with statins alone versus statins plus ezetimibe on common carotid artery intima-media thickness (CIMT) in patients with type 2 diabetes and no prior cardiovascular event.\\n2. It discusses the aggressive treatment group (target LDL-C < or = 70 mg/dl; non-high-density lipoprotein cholesterol < or = 100 mg/dl; systolic blood pressure < or = 115 mm Hg) and the standard treatment group (target LDL-C < or = 100 mg/dl; non-high-density lipoprotein cholesterol < or = 130 mg/dl; systolic blood pressure < or = 130 mm Hg).\\n3. It compares the changes in CIMT over 36 months in both aggressive and standard subgroups and finds that reducing LDL-C to aggressive targets resulted in similar regression of CIMT in patients who attained equivalent LDL-C reductions from a statin alone or statin plus ezetimibe.\\n\\nNow, let\\'s compare this information to the hypothesis:\\n\\n- Ezetimibe: The abstract mentions ezetimibe as part of a combined therapy (statins plus ezetimibe) and shows that it has similar effects on CIMT reduction as statins alone when targeting aggressive LDL-C goals.\\n- Diabetes: The abstract focuses primarily on patients with type 2 diabetes, which is relevant to the hypothesis since it involves diabetes.\\n- Therapeutic benefits or slowing disease progression: Although the abstract does not explicitly mention \"therapeutic benefits\" or \"slowing disease progression,\" the finding that reducing LDL-C to aggressive targets results in a similar CIMT regression for both statin plus ezetimibe and statin alone suggests that ezetimibe may have a role in addressing pathogenic mechanisms related to diabetes.\\n\\nBased on the analysis above, this abstract can be considered relevant for evaluating the provided hypothesis, as it provides evidence that ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes by offering similar therapeutic benefits as statins alone while reducing LDL-C levels to aggressive targets in patients with type 2 diabetes. However, it should be noted that additional information and research would be needed to fully support or refute the hypothesis.',\n",
              " \"\\n To determine if the abstract is relevant for evaluating the hypothesis, let's break down the hypothesis and the abstract into key components and compare them.\\n\\nHypothesis: ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes potentially offering therapeutic benefits or slowing disease progression.\\n\\nKey components: \\n1. Ezetimibe's potential effectiveness against diabetes pathogenic mechanisms. \\n2. Potential therapeutic benefits or slowing of disease progression due to ezetimibe use.\\n\\nNow, let's analyze the key points in the abstract:\\n1. Lowering LDL cholesterol with statin therapy reduces atherosclerotic events in many patients, but it's unclear if it benefits those with chronic kidney disease (CKD).\\n2. The study randomized CKD patients with no known history of myocardial infarction or coronary revascularization to ezetimibe 10 mg plus simvastatin 20 mg daily versus matching placebo versus simvastatin 20 mg daily.\\n3. Ezetimibe 10 mg plus simvastatin 20 mg daily was not associated with excess myopathy, hepatic toxicity, or biliary complications during follow-up. \\n4. Ezetimibe 10 mg plus simvastatin 20 mg daily yielded average LDL cholesterol differences compared with placebo, at 43 mg/dL (1.10 mmol/L) at 1 year and 33 mg/dL (0.85 mmol/L) at 2.5 years.\\n5. Follow-up continues until August 2010, with results expected for patients followed for at least 4 years.\\n6. The study aims to provide evidence about the efficacy and safety of lowering LDL cholesterol with the combination of ezetimibe and simvastatin among a wide range of patients with CKD.\\n\\nComparing these key points with the hypothesis, we can see that the abstract discusses ezetimibe's potential effectiveness when combined with another drug, simvastatin, and its effect on LDL cholesterol lowering and health outcomes among patients with CKD. However, it does not directly address ezetimibe's potential effectiveness against diabetes pathogenic mechanisms or its potential therapeutic benefits for diabetes. Therefore, this abstract does not directly support or refute the hypothesis, but it does provide some relevant information for a broader understanding of ezetimibe's effects when combined with simvastatin in patients with chronic kidney disease.\\n\\nConclusion: This abstract is somewhat relevant but does not directly support or refute the hypothesis. It provides information about ezetimibe's effects when combined with simvastatin for patients with chronic kidney disease, which may help to understand the overall impact of ezetimibe in a broader context, but it does not specifically address its potential efficacy against diabetes pathogenic mechanisms or therapeutic benefits for diabetes.\"]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cot_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7zFsrBf7ssP",
        "outputId": "c96419a1-b036-48b8-9b80-c9b7ccedf102"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are an incredibly brilliant biomedical researcher who has spent their lifetime reading all the papers in PubMed. You are focused on assisting other researchers in evaluating suggested hypotheses given abstracts in PubMed.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>\n",
              "    Hypothesis: ezetimibe may effectively alleviate or target key pathogenic mechanisms of diabetes potentially offering therapeutic benefits or slowing disease progression.\n",
              "    Abstract: PMID: 28886926 Text: In 2016, the American College of Cardiology published the first expert consensus decision pathway (ECDP) on the role of non-statin therapies for low-density lipoprotein (LDL)-cholesterol lowering in the management of atherosclerotic cardiovascular disease (ASCVD) risk. Since the publication of that document, additional evidence and perspectives have emerged from randomized clinical trials and other sources, particularly considering the longer-term efficacy and safety of proprotein convertase subtilisin/kexin 9 (PCSK9) inhibitors in secondary prevention of ASCVD. Most notably, the FOURIER (Further Cardiovascular Outcomes Research with PCSK9 Inhibition in Subjects with Elevated Risk) trial and SPIRE-1 and -2 (Studies of PCSK9 Inhibition and the Reduction of Vascular Events), assessing evolocumab and bococizumab, respectively, have published final results of cardiovascular outcomes trials in patients with clinical ASCVD and in a smaller number of high-risk primary prevention patients. In addition, further evidence on the types of patients most likely to benefit from the use of ezetimibe in addition to statin therapy after acute coronary syndrome has been published. Based on results from these important analyses, the ECDP writing committee judged that it would be desirable to provide a focused update to help guide clinicians more clearly on decision making regarding the use of ezetimibe and PCSK9 inhibitors in patients with clinical ASCVD with or without comorbidities. In the following summary table, changes from the 2016 ECDP to the 2017 ECDP Focused Update are highlighted, and a brief rationale is provided. The content of the full document has been changed accordingly, with more extensive and detailed guidance regarding decision making provided both in the text and in the updated algorithms. Revised recommendations are provided for patients with clinical ASCVD with or without comorbidities on statin therapy for secondary prevention. The ECDP writing committee judged that these new data did not warrant changes to the decision pathways and algorithms regarding the use of ezetimibe or PCSK9 inhibitors in primary prevention patients with LDL-C &lt;190 mg/dL with or without diabetes mellitus or patients without ASCVD and LDL-C ≥190 mg/dL not due to secondary causes. Based on feedback and further deliberation, the ECDP writing committee down-graded recommendations regarding bile acid sequestrant use, recommending bile acid sequestrants only as optional secondary agents for consideration in patients intolerant to ezetimibe. For clarification, the writing committee has also included new information on diagnostic categories of heterozygous and homozygous familial hypercholesterolemia, based on clinical criteria with and without genetic testing. Other changes to the original document were kept to a minimum to provide consistent guidance to clinicians, unless there was a compelling reason or new evidence, in which case justification is provided.\n",
              "\n",
              "    Determine whether or not this abstract is relevant for scientifically evaluating the provided hypothesis.\n",
              "    A relevant abstract should either support the given hypothesis or have evidence to refute the hypothesis.\n",
              "    A relevant abstract must directly comment on the hypothesis.\n",
              "\n",
              "    Let us think through this step by step.\n",
              "  </div></div></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/_t/jj8g05895j36fy27qk514qxh0000gn/T/ipykernel_5310/177203900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0massistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"chain_of_thought\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/guidance/models/_model.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;31m# run stateless functions (grammar nodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatelessFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stateless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;31m# run stateful functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/guidance/models/_model.py\u001b[0m in \u001b[0;36m_run_stateless\u001b[0;34m(lm, stateless_function, temperature, top_p, n)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mdelayed_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# last_is_generated = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mnew_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_generated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_bytes_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_group_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_token_count\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m# we make everything full probability if we are not computing uncertainty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/guidance/models/_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, grammar, max_tokens, n, top_p, temperature, ensure_bos_token)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# run a simple tokenizer (that does not use a grammar) on the prefix for better performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_byte_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_byte_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cleanup_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_byte_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_byte_positions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mpre_parser_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_byte_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/guidance/models/_model.py\u001b[0m in \u001b[0;36m_cleanup_tokens\u001b[0;34m(self, token_ids, token_byte_positions)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# compute a joint tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mjoint_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# see if we need to redo the tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/guidance/models/transformers/_transformers.py\u001b[0m in \u001b[0;36m_joint_tokenize\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_joint_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mfirst_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mnew_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_decode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3752\u001b[0m         \"\"\"\n\u001b[1;32m   3753\u001b[0m         \u001b[0;31m# Convert inputs to python lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3754\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3756\u001b[0m         return self._decode(\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mframework_to_test_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_frameworks_and_test_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframework_to_test_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtest_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mframework_to_py_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mis_tf_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mTests\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSafe\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcall\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m_is_tensorflow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_current_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/keras/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/keras/api/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/keras/api/keras/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/keras/api/keras/datasets/imdb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_word_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/keras/datasets/imdb.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_remove_long_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/keras/preprocessing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \"\"\"\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndimage\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/scipy/linalg/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m \"\"\"  # noqa: E501\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_misc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_cythonized_array_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_basic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/scipy/linalg/_misc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mblas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/scipy/linalg/blas.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fblas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_cblas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with system():\n",
        "    lm = mistral + sys_prompt\n",
        "\n",
        "with user():\n",
        "    lm += prompt\n",
        "\n",
        "with assistant():\n",
        "    lm += gen(max_tokens = 500, temperature = config.temperature, name = \"chain_of_thought\")\n",
        "\n",
        "with user():\n",
        "    lm += \"Give a score of either 0: (Not relevant) or 1: (Relevant) for the above abstract. Answer: \" + select([0, 1], name = \"answer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc5Z06Fq7ssP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "komPydZQ7ssP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJXlWKf57ssP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
