
==========
== CUDA ==
==========

CUDA Version 12.1.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Running job on gpu4002.chtc.wisc.edu
GPUs assigned: GPU-3cc0e7db
Number of arguments received: 0
Arguments received: 
First argument ($1): ''
Second argument ($2): ''
--- Network Connectivity Test (using curl) ---
Mon Apr  7 15:18:08 UTC 2025
--- Testing Google (HTTP) (URL: http://google.com) ---
--- Raw curl -Is output for Google (HTTP): ---
HTTP/1.1 301 Moved Permanently
Location: http://www.google.com/
Content-Type: text/html; charset=UTF-8
Content-Security-Policy-Report-Only: object-src 'none';base-uri 'self';script-src 'nonce-Rns4ib_B5zYO516-8SI0bw' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp
Expires: Wed, 16 Apr 2025 15:51:15 GMT
Cache-Control: public, max-age=2592000
Server: gws
Content-Length: 219
X-XSS-Protection: 0
X-Frame-Options: SAMEORIGIN
Age: 1812413
Date: Mon, 17 Mar 2025 15:51:15 GMT
Warning: 113 squid/frontier-squid-5.9-3.2.osg23.el8 "This cache hit is still fresh and more than 1 day old"
X-Cache: HIT from squid2003
Via: 1.1 squid2003 (squid/frontier-squid-5.9-3.2.osg23.el8)
Connection: keep-alive

--- End raw curl output for Google (HTTP) ---
--- Testing PubMed E-utilities (HTTPS) (URL: https://eutils.ncbi.nlm.nih.gov) ---
--- Raw curl -Is output for PubMed E-utilities (HTTPS): ---
HTTP/2 301 
strict-transport-security: max-age=31536000; includeSubDomains; preload
content-security-policy: upgrade-insecure-requests
referrer-policy: origin-when-cross-origin
location: https://www.ncbi.nlm.nih.gov/books/NBK25501/
content-type: text/html; charset=iso-8859-1
date: Mon, 07 Apr 2025 15:18:08 GMT
server: Apache

--- End raw curl output for PubMed E-utilities (HTTPS) ---
--- Testing Hugging Face (HTTPS) (URL: https://huggingface.co) ---
--- Raw curl -Is output for Hugging Face (HTTPS): ---
HTTP/2 200 
content-type: text/html; charset=utf-8
content-length: 129391
date: Mon, 07 Apr 2025 15:17:51 GMT
x-powered-by: huggingface-moon
cross-origin-opener-policy: same-origin
referrer-policy: strict-origin-when-cross-origin
x-request-id: Root=1-67f3ec9e-668d9bb810a8798703fcd597
x-frame-options: DENY
etag: W/"1f96f-tG/oYulrpzN6I8n5pErhd/x64DY"
vary: Accept-Encoding
x-cache: Hit from cloudfront
via: 1.1 070752a16025100a5beaef571e0d3ef6.cloudfront.net (CloudFront)
x-amz-cf-pop: ORD56-P7
x-amz-cf-id: j0J4fVc7OOCVbuEf7OyME-UGg4GvWOCS-oX_acrQh-uyRPcOBIsxVA==
age: 17

--- End raw curl output for Hugging Face (HTTPS) ---
--- End Network Connectivity Test ---
--- HTCondor Environment Variables ---
_CONDOR_ITEM (env var): 
_CONDOR_JOBID (env var): 
_CONDOR_CLUSTERID (env var): 
_CONDOR_PROCID (env var): 
_CONDOR_JOBAD_RAW (env var): 
--- End of HTCondor Environment Variables ---
Job type detected. Running km_with_gpt.
INFO 04-07 15:18:30 llm_engine.py:87] Initializing an LLM engine with config: model='lexu14/porpoise1', tokenizer='lexu14/porpoise1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4000, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 04-07 15:18:33 weight_utils.py:163] Using model weights format ['*.safetensors']
INFO 04-07 15:18:55 llm_engine.py:357] # GPU blocks: 5485, # CPU blocks: 682
INFO 04-07 15:18:57 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-07 15:18:57 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-07 15:19:04 model_runner.py:756] Graph capturing finished in 7 secs.
Warning: No breakpoint was specified.
Warning: No breakpoint was specified.
