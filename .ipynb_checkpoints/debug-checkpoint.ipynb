{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509f3a84-1564-4f3d-aea0-a96793eb345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b49a644-63e3-46a7-99d0-de62efa8fcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "            max_seq_length = 2048,\n",
    "            load_in_4bit = True,\n",
    "            trust_remote_code = True,\n",
    "            attn_implementation = 'flash_attention_2',\n",
    "            device_map = \"auto\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875e57f4-4c82-4083-9821-bd67a6bf0104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('model',\n",
       "               MistralModel(\n",
       "                 (embed_tokens): Embedding(32064, 3072)\n",
       "                 (layers): ModuleList(\n",
       "                   (0-31): 32 x MistralDecoderLayer(\n",
       "                     (self_attn): MistralAttention(\n",
       "                       (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                       (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                       (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                       (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                       (rotary_emb): LlamaRotaryEmbedding()\n",
       "                     )\n",
       "                     (mlp): MistralMLP(\n",
       "                       (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                       (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                       (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                       (act_fn): SiLU()\n",
       "                     )\n",
       "                     (input_layernorm): MistralRMSNorm()\n",
       "                     (post_attention_layernorm): MistralRMSNorm()\n",
       "                   )\n",
       "                 )\n",
       "                 (norm): MistralRMSNorm()\n",
       "               )),\n",
       "              ('lm_head',\n",
       "               Linear(in_features=3072, out_features=32064, bias=False))]),\n",
       " 'config': MistralConfig {\n",
       "   \"_name_or_path\": \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\",\n",
       "   \"architectures\": [\n",
       "     \"MistralForCausalLM\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0.0,\n",
       "   \"bos_token_id\": 1,\n",
       "   \"eos_token_id\": 32000,\n",
       "   \"hidden_act\": \"silu\",\n",
       "   \"hidden_size\": 3072,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 8192,\n",
       "   \"max_position_embeddings\": 4096,\n",
       "   \"model_type\": \"mistral\",\n",
       "   \"num_attention_heads\": 32,\n",
       "   \"num_hidden_layers\": 32,\n",
       "   \"num_key_value_heads\": 32,\n",
       "   \"pad_token_id\": 32000,\n",
       "   \"quantization_config\": {\n",
       "     \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "     \"bnb_4bit_quant_type\": \"nf4\",\n",
       "     \"bnb_4bit_use_double_quant\": true,\n",
       "     \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "     \"llm_int8_has_fp16_weight\": false,\n",
       "     \"llm_int8_skip_modules\": null,\n",
       "     \"llm_int8_threshold\": 6.0,\n",
       "     \"load_in_4bit\": true,\n",
       "     \"load_in_8bit\": false,\n",
       "     \"quant_method\": \"bitsandbytes\"\n",
       "   },\n",
       "   \"rms_norm_eps\": 1e-05,\n",
       "   \"rope_scaling\": null,\n",
       "   \"rope_theta\": 10000.0,\n",
       "   \"sliding_window\": 2048,\n",
       "   \"tie_word_embeddings\": false,\n",
       "   \"torch_dtype\": \"bfloat16\",\n",
       "   \"transformers_version\": \"4.42.3\",\n",
       "   \"unsloth_version\": \"2024.7\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 32064\n",
       " },\n",
       " 'name_or_path': 'unsloth/phi-3-mini-4k-instruct-bnb-4bit',\n",
       " 'warnings_issued': {},\n",
       " 'generation_config': GenerationConfig {\n",
       "   \"bos_token_id\": 1,\n",
       "   \"eos_token_id\": 32000,\n",
       "   \"pad_token_id\": 32000\n",
       " },\n",
       " '_keep_in_fp32_modules': None,\n",
       " 'vocab_size': 32064,\n",
       " 'is_quantized': True,\n",
       " 'quantization_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " '_is_hf_initialized': True,\n",
       " '_old_forward': <bound method MistralForCausalLM_fast_forward of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " '_hf_hook': AlignDevicesHook(execution_device=0, offload=False, io_same_device=True, offload_buffers=False, place_submodules=True, skip_keys='past_key_values'),\n",
       " 'forward': functools.partial(<function add_hook_to_module.<locals>.new_forward at 0x7fcd15a88dc0>, MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )),\n",
       " 'to': <function torch.nn.modules.module.Module.to(*args, **kwargs)>,\n",
       " 'cuda': <function torch.nn.modules.module.Module.cuda(device: Union[int, torch.device, NoneType] = None) -> ~T>,\n",
       " 'hf_device_map': {'': 0},\n",
       " 'is_loaded_in_4bit': True,\n",
       " 'is_4bit_serializable': True,\n",
       " 'hf_quantizer': <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer at 0x7fcd068de4a0>,\n",
       " 'max_seq_length': 4096,\n",
       " 'original_push_to_hub': <bound method PushToHubMixin.push_to_hub of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " 'push_to_hub': <bound method unsloth_push_to_hub of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " 'model_tags': ['unsloth'],\n",
       " 'push_to_hub_merged': <bound method unsloth_push_to_hub_merged of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " 'save_pretrained_merged': <bound method unsloth_save_pretrained_merged of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " 'push_to_hub_gguf': <bound method unsloth_push_to_hub_gguf of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " 'save_pretrained_gguf': <bound method unsloth_save_pretrained_gguf of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " 'push_to_hub_ggml': <bound method unsloth_convert_lora_to_ggml_and_push_to_hub of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " 'save_pretrained_ggml': <bound method unsloth_convert_lora_to_ggml_and_save_locally of MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(32064, 3072)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x MistralDecoderLayer(\n",
       "         (self_attn): MistralAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm()\n",
       "         (post_attention_layernorm): MistralRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       " )>,\n",
       " '_saved_temp_tokenizer': LlamaTokenizerFast(name_or_path='unsloth/phi-3-mini-4k-instruct-bnb-4bit', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|placeholder6|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cf136-365d-476b-89f0-ce714994db35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
